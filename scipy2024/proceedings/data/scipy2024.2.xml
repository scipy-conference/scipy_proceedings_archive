<doi_batch xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="5.3.1" xmlns="http://www.crossref.org/schema/5.3.1" xmlns:jats="http://www.ncbi.nlm.nih.gov/JATS1" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:fr="http://www.crossref.org/fundref.xsd" xmlns:ai="http://www.crossref.org/AccessIndicators.xsd" xsi:schemaLocation="http://www.crossref.org/schema/5.3.1 http://www.crossref.org/schemas/crossref5.3.1.xsd"><head><doi_batch_id>3151ac41-469d-4b92-a0ff-cc400331752c</doi_batch_id><timestamp>1730770000944</timestamp><depositor><depositor_name>Curvenote</depositor_name><email_address>doi@curvenote.com</email_address></depositor><registrant>Crossref</registrant></head><body><conference><contributors><person_name sequence="first" contributor_role="editor"><given_name>Meghann</given_name><surname>Agarwal</surname><affiliations><institution><institution_name>GDI</institution_name></institution></affiliations><alt-name><string-name>Meghann Agarwal</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="editor"><given_name>Amey</given_name><surname>Ambade</surname><affiliations><institution><institution_name>SLB</institution_name></institution></affiliations><alt-name><string-name>Amey Ambade</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="editor"><given_name>Chris</given_name><surname>Calloway</surname><affiliations><institution><institution_name>University of North Carolina</institution_name></institution></affiliations><alt-name><string-name>Chris Calloway</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="editor"><given_name>Rowan</given_name><surname>Cockett</surname><affiliations><institution><institution_name>Curvenote</institution_name></institution></affiliations><alt-name><string-name>Rowan Cockett</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="editor"><given_name>Sanhita</given_name><surname>Joshi</surname><affiliations><institution><institution_name>Deloitte</institution_name></institution></affiliations><alt-name><string-name>Sanhita Joshi</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="editor"><given_name>Charles</given_name><surname>Lindsey</surname><affiliations><institution><institution_name>Aptos</institution_name></institution></affiliations><alt-name><string-name>Charles Lindsey</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="editor"><given_name>Hongsup</given_name><surname>Shin</surname><affiliations><institution><institution_name>Arm</institution_name></institution></affiliations><alt-name><string-name>Hongsup Shin</string-name></alt-name></person_name></contributors><event_metadata><conference_name>Python in Science Conference</conference_name><conference_acronym>SciPy</conference_acronym><conference_number>23rd</conference_number><conference_location>Tacoma, Washington</conference_location><conference_date>July 8 - July 14, 2024</conference_date></event_metadata><proceedings_series_metadata><series_metadata><titles><title>Proceedings of the Python in Science Conference</title><original_language_title>Proceedings of the Python in Science Conference</original_language_title></titles><issn>2575-9752</issn><doi_data><doi>10.25080/issn.2575-9752</doi><resource>https://doi.curvenote.com/10.25080/issn.2575-9752</resource></doi_data></series_metadata><proceedings_title>Proceedings of the 23rd Python in Science Conference</proceedings_title><proceedings_subject>Scientific Computing with Python</proceedings_subject><publisher><publisher_name>SciPy</publisher_name></publisher><publication_date media_type="online"><month>07</month><day>10</day><year>2024</year></publication_date><noisbn reason="simple_series"></noisbn><doi_data><doi>10.25080/DTVR3553</doi><resource>https://doi.curvenote.com/10.25080/DTVR3553</resource></doi_data></proceedings_series_metadata><conference_paper><contributors><person_name sequence="first" contributor_role="author"><given_name>Alan</given_name><surname>Lujan</surname><affiliations><institution><institution_name>Johns Hopkins University</institution_name><institution_id type="ror">https://ror.org/00za53h95</institution_id><institution_department>Department of Economics</institution_department></institution><institution><institution_name>Econ-ARK</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0002-5289-7054</ORCID><alt-name><string-name>Alan Lujan</string-name></alt-name></person_name></contributors><titles><title>multinterp</title><subtitle>A Unified Interface for Multivariate Interpolation in the Scientific Python Ecosystem</subtitle></titles><jats:abstract><jats:p>Multivariate interpolation is a fundamental tool in scientific computing used to approximate the values of a function between known data points in multiple dimensions. Despite its importance, the Python ecosystem offers a fragmented landscape of specialized tools for this task. This fragmentation hinders code reusability, experimentation, and efficient deployment across diverse hardware. The <jats:monospace>multinterp</jats:monospace> package was developed to address this challenge. It provides a unified interface for various grids used for interpolation (regular, irregular, and curvilinear), supports multiple backends (<jats:abbrev alt="Central Processing Unit">CPU</jats:abbrev>, parallel, and <jats:abbrev alt="Graphics Processing Unit">GPU</jats:abbrev>), and includes tools for multivalued interpolation and interpolation of derivatives. This paper introduces <jats:monospace>multinterp</jats:monospace>, demonstrates its capabilities, and invites the community to contribute to its development.</jats:p></jats:abstract><publication_date media_type="online"><month>07</month><day>10</day><year>2024</year></publication_date><pages><first_page>1</first_page><last_page>19</last_page></pages><ai:program name="AccessIndicators"><ai:free_to_read></ai:free_to_read><ai:license_ref applies_to="vor">https://creativecommons.org/licenses/by/4.0/</ai:license_ref></ai:program><doi_data><doi>10.25080/FGCJ9164</doi><resource content_version="vor">https://doi.curvenote.com/10.25080/FGCJ9164</resource></doi_data><citation_list><citation key="Oliphant2007"><doi>10.1109/MCSE.2007.58</doi></citation><citation key="Harris2020"><doi>10.1038/s41586-020-2649-2</doi></citation><citation key="Virtanen2020"><doi>10.1038/s41592-019-0686-2</doi></citation><citation key="Lam2015"><doi>10.1145/2833157.2833162</doi></citation></citation_list></conference_paper><conference_paper><contributors><person_name sequence="first" contributor_role="author"><given_name>Aleksandar</given_name><surname>Makelov</surname><affiliations><institution><institution_name>MIT</institution_name></institution><institution><institution_name>SERI MATS</institution_name></institution><institution><institution_name>Independent Researcher</institution_name></institution></affiliations><ORCID>https://orcid.org/0009-0003-5394-112X</ORCID><alt-name><string-name>Aleksandar Makelov</string-name></alt-name></person_name></contributors><titles><title>Mandala: Compositional Memoization for Simple &#x26; Powerful Scientific Data Management</title></titles><jats:abstract><jats:p>We present <jats:ext-link ext-link-type="uri" xlink:href="https://github.com/amakelov/mandala"><jats:monospace>mandala</jats:monospace></jats:ext-link>, a Python library that largely eliminates the accidental complexity of scientific data management and incremental computing. While most traditional and/or popular data management solutions are based on <jats:italic>logging</jats:italic>, <jats:monospace>mandala</jats:monospace> takes a fundamentally different approach, using <jats:italic>memoization</jats:italic> of function calls as the fundamental unit of saving, loading, querying and deleting computational artifacts.</jats:p><jats:p>It does so by implementing a <jats:italic>compositional</jats:italic> form of memoization, which keeps track of how memoized functions compose with one another. In this way: (1) complex computations are effectively memoized end-to-end, and become ‘interfaces’ to their own intermediate results by retracing the memoized calls; (2) all computations in a project form a single computational graph, which can be explored, queried and manipulated in high-level ways through a <jats:italic>computation frame</jats:italic>, which is a natural generalization of a dataframe that replaces columns by a computation graph, and rows by (partial) executions of this graph.</jats:p><jats:p>Several features implemented on top of the core memoization data structures — such as natively and transparently handling Python collections, in-memory caching of intermediate results, and a flexible versioning system with dynamic dependency tracking — turn <jats:monospace>mandala</jats:monospace> into a practical and simple tool for managing and interacting with computational data.</jats:p></jats:abstract><publication_date media_type="online"><month>07</month><day>10</day><year>2024</year></publication_date><pages><first_page>20</first_page><last_page>28</last_page></pages><ai:program name="AccessIndicators"><ai:free_to_read></ai:free_to_read><ai:license_ref applies_to="vor">https://creativecommons.org/licenses/by/4.0/</ai:license_ref></ai:program><doi_data><doi>10.25080/JHPV7385</doi><resource content_version="vor">https://doi.curvenote.com/10.25080/JHPV7385</resource></doi_data><citation_list><citation key="hey2009fourth"><doi>10.1007/978-3-642-33299-9_1</doi></citation><citation key="ravi2016deep"><doi>10.1109/JBHI.2016.2636665</doi></citation><citation key="abramson2024accurate"><doi>10.1038/s41586-024-07487-w</doi></citation><citation key="wickham2014tidy"><doi>10.18637/jss.v059.i10</doi></citation><citation key="davidson2008provenance"><doi>10.1145/1376616.1376772</doi></citation><citation key="ivie2018reproducibility"><doi>10.1145/3186266</doi></citation><citation key="sandve2013ten"><doi>10.1371/journal.pcbi.1003285</doi></citation><citation key="wilkinson2016fair"><doi>10.1038/sdata.2016.18</doi></citation><citation key="Brooks1987NoSB"><doi>10.1109/MC.1987.1663532</doi></citation><citation key="guo2011using"><doi>10.1145/2001420.2001455</doi></citation><citation key="lavigne2021funsies"><doi>10.21105/joss.03274</doi></citation><citation key="codd1970relational"><doi>10.1145/362384.362685</doi></citation><citation key="patterson2022categorical"><doi>10.32408/compositionality-4-5</doi></citation><citation key="abadi2016tensorflow"><doi>10.5281/zenodo.4724125</doi></citation></citation_list></conference_paper><conference_paper><contributors><person_name sequence="first" contributor_role="author"><given_name>Seyed Alireza</given_name><surname>Vaezi</surname><affiliations><institution><institution_name>University of Georgia</institution_name><institution_id type="ror">https://ror.org/00te3t702</institution_id></institution></affiliations><ORCID>https://orcid.org/0009-0000-2089-8362</ORCID><alt-name><string-name>Seyed Alireza Vaezi</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Shannon</given_name><surname>Quinn</surname><affiliations><institution><institution_name>University of Georgia</institution_name><institution_id type="ror">https://ror.org/00te3t702</institution_id></institution></affiliations><alt-name><string-name>Shannon Quinn</string-name></alt-name></person_name></contributors><titles><title>Training a Supervised Cilia Segmentation Model from Self-Supervision</title></titles><jats:abstract><jats:p>Cilia are organelles found on the surface of some cells in the human body that sweep rhythmically to transport substances. Dysfunctional cilia are indicative of diseases that can disrupt organs such as the lungs and kidneys. Understanding cilia behavior is essential in diagnosing and treating such diseases. But, the tasks of automatically analyzing cilia are often a labor and time-intensive since there is a lack of automated segmentation. In this work we overcome this bottleneck by developing a robust, self-supervised framework exploiting the visual similarity of normal and dysfunctional cilia. This framework generates pseudolabels from optical flow motion vectors, which serve as training data for a semi-supervised neural network. Our approach eliminates the need for manual annotations, enabling accurate and efficient segmentation of both motile and immotile cilia.</jats:p></jats:abstract><publication_date media_type="online"><month>07</month><day>10</day><year>2024</year></publication_date><pages><first_page>29</first_page><last_page>38</last_page></pages><ai:program name="AccessIndicators"><ai:free_to_read></ai:free_to_read><ai:license_ref applies_to="vor">https://creativecommons.org/licenses/by/4.0/</ai:license_ref></ai:program><doi_data><doi>10.25080/HXCJ6205</doi><resource content_version="vor">https://doi.curvenote.com/10.25080/HXCJ6205</resource></doi_data><citation_list><citation key="Hoyer-Fender2013"><doi>10.1007/978-94-007-5808-7_1</doi></citation><citation key="Hansen2021-fd"><doi>10.1140/epje/s10189-021-00031-y</doi></citation><citation key="lee2011muco"><doi>10.1016/j.compfluid.2011.05.016</doi></citation><citation key="zain2022low"><doi>10.25080/majora-212e5952-026</doi></citation><citation key="zain2020towards"><doi>10.25080/Majora-342d178e-017</doi></citation><citation key="lu2018stacked"><doi>10.48550/arXiv.1803.07534</doi></citation><citation key="https://doi.org/10.1002/ppul.24078"><doi>10.1002/ppul.24078</doi></citation><citation key="vaezi2022novel"><doi>10.25080/majora-212e5952-009</doi></citation><citation key="quinn2015automated"><doi>10.1126/scitranslmed.aaa1233</doi></citation><citation key="van2020survey"><doi>10.1007/s10994-019-05855-6</doi></citation><citation key="10.3389/fcvm.2020.00105"><doi>10.3389/fcvm.2020.00105</doi></citation><citation key="Krois2021"><doi>10.1038/s41598-021-85454-5</doi></citation><citation key="10.1148/ryai.2020190195"><doi>10.1148/ryai.2020190195</doi></citation><citation key="Sandfort2019"><doi>10.1016/j.imu.2021.100779</doi></citation><citation key="YAKIMOVICH2021100383"><doi>10.1016/j.patter.2021.100383</doi></citation><citation key="van2001art"><doi>10.1198/10618600152418584</doi></citation><citation key="krizhevsky2012imagenet"><doi>10.1145/3065386</doi></citation><citation key="ronneberger2015u"><doi>10.48550/arXiv.1505.04597</doi></citation><citation key="goodfellow2014generative"><doi>10.48550/arXiv.1406.2661</doi></citation><citation key="yi2019generative"><doi>10.48550/arXiv.1809.07294</doi></citation><citation key="Sanford2020-yg"><doi>10.2214/AJR.19.22347</doi></citation><citation key="NEURIPS2019_eb1e7832"><doi>10.48550/arXiv.1902.07208</doi></citation><citation key="hutchinson2017overcoming"><doi>10.48550/arXiv.1711.05099</doi></citation><citation key="kim2019self"><doi>10.48550/arXiv.1811.09795</doi></citation><citation key="kolesnikov2019revisiting"><doi>10.48550/arXiv.1901.09005</doi></citation><citation key="mahendran2019cross"><doi>10.48550/arXiv.1807.05636</doi></citation><citation key="li2006one"><doi>10.1109/TPAMI.2006.79</doi></citation><citation key="miller2000learning"><doi>10.1109/CVPR.2000.855856</doi></citation><citation key="khatibi2021proposing"><doi>10.1111/srt.12920</doi></citation><citation key="doretto2003dynamic"><doi>10.1023/A:1021669406132</doi></citation><citation key="Hyndman2007HigherorderAM"><doi>10.5244/C.21.76</doi></citation><citation key="kirillov2017unified"><doi>10.48550/arXiv.2112.04603</doi></citation></citation_list></conference_paper><conference_paper><contributors><person_name sequence="first" contributor_role="author"><given_name>Amadi Gabriel</given_name><surname>Udu</surname><affiliations><institution><institution_name>School of Engineering, University of Leicester, LE1 7RH, Leicester, UK.</institution_name></institution><institution><institution_name>Air Force Institute of Technology, Air Force Base, PMB 2104,  Kaduna, Nigeria.</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0001-8944-4940</ORCID><alt-name><string-name>Amadi Gabriel Udu</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Andrea</given_name><surname>Lecchini-Visintini</surname><affiliations><institution><institution_name>School of Electronics and Computer Science, University of Southampton, SO17 1BJ, Southampton, UK.</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0002-1654-8877</ORCID><alt-name><string-name>Andrea Lecchini-Visintini</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Steve R.</given_name><surname>Gunn</surname><affiliations><institution><institution_name>School of Electronics and Computer Science, University of Southampton, SO17 1BJ, Southampton, UK.</institution_name></institution></affiliations><alt-name><string-name>Steve R. Gunn</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Norman</given_name><surname>Osa-uwagboe</surname><affiliations><institution><institution_name>Wolfson School of Mechanical, Electrical, and Manufacturing Engineering, Loughborough University, LE11 3TU, Loughborough, UK.</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0002-6560-445X</ORCID><alt-name><string-name>Norman Osa-uwagboe</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Maryam Khaksar</given_name><surname>Ghalati</surname><affiliations><institution><institution_name>School of Engineering, University of Leicester, LE1 7RH, Leicester, UK.</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0002-6009-7946</ORCID><alt-name><string-name>Maryam Khaksar Ghalati</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Hongbiao</given_name><surname>Dong</surname><affiliations><institution><institution_name>School of Engineering, University of Leicester, LE1 7RH, Leicester, UK.</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0003-1244-0364</ORCID><alt-name><string-name>Hongbiao Dong</string-name></alt-name></person_name></contributors><titles><title>Computational Resource Optimisation in Feature Selection under Class Imbalance Conditions</title></titles><jats:abstract><jats:p>Feature selection is crucial for reducing data dimensionality as well as enhancing model interpretability and performance in machine learning tasks. However, selecting the most informative features in large dataset often incurs high computational costs. This study explores the possibility of performing feature selection on a subset of data to reduce the computational burden. The study uses five real-life datasets with substantial sample sizes and severe class imbalance ratios between 0.09 – 0.18. The results illustrate the variability of feature importance with smaller sample fractions in different models. In this cases considered, light gradient-boosting machine exhibited the least variability, even with reduced sample fractions, while also incurring the least computational resource.</jats:p></jats:abstract><publication_date media_type="online"><month>07</month><day>10</day><year>2024</year></publication_date><pages><first_page>39</first_page><last_page>46</last_page></pages><ai:program name="AccessIndicators"><ai:free_to_read></ai:free_to_read><ai:license_ref applies_to="vor">https://creativecommons.org/licenses/by/4.0/</ai:license_ref></ai:program><doi_data><doi>10.25080/TPGN6857</doi><resource content_version="vor">https://doi.curvenote.com/10.25080/TPGN6857</resource></doi_data><citation_list><citation key="Cai2018"><doi>10.1016/j.neucom.2017.11.077</doi></citation><citation key="Dhal2022"><doi>10.1007/s10489-021-02550-9</doi></citation><citation key="Udu2023a"><doi>10.1007/978-3-031-39847-6_42</doi></citation><citation key="Yin2013"><doi>10.1016/j.neucom.2012.04.039</doi></citation><citation key="Tsai2020"><doi>10.1016/j.knosys.2020.106097</doi></citation><citation key="deHaro-Garcia2020"><doi>10.1016/j.ins.2020.05.077</doi></citation><citation key="Matharaarachchi2021"><doi>10.1016/j.mlwa.2021.100170</doi></citation><citation key="feng2019"><doi>10.1177/07316844241236696</doi></citation><citation key="sarker2021"><doi>10.1007/s42979-021-00592-x</doi></citation><citation key="paleyes2022"><doi>10.1145/3533378</doi></citation><citation key="udu2024a"><doi>10.1177/07316844241236696</doi></citation><citation key="Luque2019"><doi>10.1016/j.patcog.2019.02.023</doi></citation><citation key="Temraz2022"><doi>10.1016/j.mlwa.2022.100375</doi></citation><citation key="pandas1"><doi>10.5281/zenodo.3509134</doi></citation><citation key="numpy"><doi>10.1038/s41586-020-2649-2</doi></citation><citation key="matplotlib"><doi>10.1109/MCSE.2007.55</doi></citation><citation key="scipy"><doi>10.1038/s41592-019-0686-2</doi></citation><citation key="diabetes"><doi>10.24432/C53919</doi></citation><citation key="census_income"><doi>10.24432/C5GP7S</doi></citation><citation key="bank_marketing"><doi>10.24432/C5K306</doi></citation><citation key="statlog"><doi>10.24432/C5WS31</doi></citation><citation key="Osa-uwagboe2024"><doi>10.3390/ma17112549</doi></citation><citation key="Udu2023b"><doi>10.1109/ICMLA58977.2023.00159</doi></citation><citation key="rezvani2023"><doi>10.1016/j.asoc.2023.110415</doi></citation><citation key="Udu2024b"><doi>10.1109/CONTROL60310.2024.10531841</doi></citation><citation key="Li2017"><doi>10.1145/3136625</doi></citation><citation key="Kaneko2022"><doi>10.1002/ansa.202200018</doi></citation></citation_list></conference_paper><conference_paper><contributors><person_name sequence="first" contributor_role="author"><given_name>Arushi</given_name><surname>Nath</surname><affiliations><institution><institution_name>Founder, MonitorMyPlanet.com</institution_name></institution></affiliations><ORCID>https://orcid.org/0009-0007-5329-9148</ORCID><alt-name><string-name>Arushi Nath</string-name></alt-name></person_name></contributors><titles><title>Algorithms to Determine Asteroid’s Physical Properties using Sparse and Dense Photometry, Robotic Telescopes and Open Data</title></titles><jats:abstract><jats:p>The rapid pace of discovering asteroids due to advancements in detection techniques outpaces current abilities to analyze them comprehensively. Understanding an asteroid’s physical properties is crucial for effective deflection strategies and improves our understanding of the solar system’s formation and evolution. Dense photometry provides continuous time-series measurements valuable for determining an asteroid’s rotation period, yet is limited to a singular phase angle. Conversely, sparse photometry offers non-continuous measurements across multiple phase angles, essential for determining an asteroid’s absolute magnitude, albedo (reflectivity), and size. This paper presents open-source algorithms that integrate dense photometry from citizen scientists with sparse photometry from space and ground-based all-sky surveys to determine asteroids’ albedo, size, rotation, strength, and composition.
Applying the algorithms to the Didymos binary asteroid, combined with data from GAIA, the Zwicky Transient Facility, and <jats:abbrev alt="Asteroid Terrestrial-impact Last Alert System">ATLAS</jats:abbrev> photometric sky surveys, revealed Didymos to be 840 meters wide, with a 0.14 albedo, an 18.14 absolute magnitude, a 2.26-hour rotation period, rubble-pile strength, and an <jats:abbrev alt="siliceous">S-type</jats:abbrev> composition. Didymos was the target of the 2022 NASA Double Asteroid Redirection Test (<jats:abbrev alt="Double Asteroid Redirection Test">DART</jats:abbrev>) mission. The algorithm successfully measured a 35-minute decrease in the mutual orbital period following the <jats:abbrev alt="Double Asteroid Redirection Test">DART</jats:abbrev> mission, equating to a 40-meter reduction in the mutual orbital radius, proving a successful deflection. Analysis of the broader asteroid population highlighted significant compositional diversity, with a predominance of carbonaceous (<jats:abbrev alt="carbonaceous">C-type</jats:abbrev>) asteroids in the outer regions of the asteroid belt and siliceous (<jats:abbrev alt="siliceous">S-type</jats:abbrev>) and metallic (<jats:abbrev alt="metallic">M-type</jats:abbrev>) asteroids more common in the inner regions. These findings provide insights into the diversity and distribution of asteroid compositions, reflecting the conditions and processes of the early solar system.
This work empowers citizen scientists to become planetary defenders, contributing significantly to planetary defense and enhancing our understanding of solar system composition and evolution.</jats:p></jats:abstract><publication_date media_type="online"><month>07</month><day>10</day><year>2024</year></publication_date><pages><first_page>47</first_page><last_page>55</last_page></pages><ai:program name="AccessIndicators"><ai:free_to_read></ai:free_to_read><ai:license_ref applies_to="vor">https://creativecommons.org/licenses/by/4.0/</ai:license_ref></ai:program><doi_data><doi>10.25080/TWCF2755</doi><resource content_version="vor">https://doi.curvenote.com/10.25080/TWCF2755</resource></doi_data><citation_list><citation key="Shevchenko2019"><doi>10.1051/0004-6361/201935588</doi></citation><citation key="Pravec2000"><doi>10.1006/icar.2000.6482</doi></citation><citation key="DeMeo2014"><doi>10.1038/nature12908</doi></citation></citation_list></conference_paper><conference_paper><contributors><person_name sequence="first" contributor_role="author"><given_name>Atharva</given_name><surname>Rasane</surname><affiliations><institution><institution_name>KLE Technology University</institution_name></institution></affiliations><alt-name><string-name>Atharva Rasane</string-name></alt-name></person_name></contributors><titles><title>AI-Driven Watermarking Technique for Safeguarding Text Integrity in the Digital Age</title></titles><jats:abstract><jats:p>The internet’s growth has led to a surge in text usage. Now, with public access to generative <jats:abbrev alt="Artificial Intelligence">AI</jats:abbrev> models like ChatGPT/Bard, identifying the source is vital. This is crucial due to concerns about copyright infringement and plagiarism. Moreover, it is essential to differentiate <jats:abbrev alt="Artificial Intelligence">AI</jats:abbrev>-generated text to curb misinformation from <jats:abbrev alt="Artificial Intelligence">AI</jats:abbrev> model hallucinations.</jats:p><jats:p>In this paper, we explore text watermarking as a potential solution, focusing on plain ASCII text in English. We investigate techniques including physical watermarking (e.g., UniSpaCh by Por et al.), which modifies text to hide a binary message using Unicode Spaces, and logical watermarking (e.g., word context by Jalil et al.), which generates a watermark key via a defined process. While logical watermarking is difficult to break but undetectable without prior knowledge, physical watermarks are easily detected but also easy to break.</jats:p><jats:p>This paper presents a unique physical watermarking technique based on word substitution to address these challenges. The core idea is that <jats:abbrev alt="Artificial Intelligence">AI</jats:abbrev> models consistently produce the same output for the same input. Initially, we replaced every i-th word (for example, every 5th word) with a “[MASK],” a placeholder token used in natural language processing models to indicate where a word has been removed and needs to be predicted. Then, we used a <jats:abbrev alt="Bidirectional Encoder Representations from Transformers">BERT</jats:abbrev> model to predict the most probable token in place of “[MASK].” The resulting text constitutes the watermarked text. To verify, we reran the algorithm on the watermarked text and compared the input and output for similarity.</jats:p><jats:p>The Python implementation of the algorithm in this paper employs models from the HuggingFace Transformer Library, namely “bert-base-uncased” and “distilroberta-base”. The “[MASK]” placeholder was generated by splitting the input string using the split() function and then replacing every 5th element in the list with “[MASK]”. This modified list served as the input text for the <jats:abbrev alt="Bidirectional Encoder Representations from Transformers">BERT</jats:abbrev> model, where the output corresponding to each “[MASK]” was replaced accordingly. Finally, applying the join() function to the list produces the watermarked text.</jats:p><jats:p>This technique tends to generate nearly invisible watermarked text, preserving its integrity or completely changing the meaning of the text based on how similar the text is to the training dataset of <jats:abbrev alt="Bidirectional Encoder Representations from Transformers">BERT</jats:abbrev>. This was observed when the algorithm was run on the story of Red Riding Hood, where its meaning was altered. However, the nature of this watermark makes it extremely difficult to break due to the black-box nature of the <jats:abbrev alt="Artificial Intelligence">AI</jats:abbrev> model.</jats:p></jats:abstract><publication_date media_type="online"><month>07</month><day>10</day><year>2024</year></publication_date><pages><first_page>56</first_page><last_page>77</last_page></pages><ai:program name="AccessIndicators"><ai:free_to_read></ai:free_to_read><ai:license_ref applies_to="vor">https://creativecommons.org/licenses/by/4.0/</ai:license_ref></ai:program><doi_data><doi>10.25080/DHKD1726</doi><resource content_version="vor">https://doi.curvenote.com/10.25080/DHKD1726</resource></doi_data><citation_list><citation key="Atr01"><doi>10.1109/ACCESS.2018.2796585</doi></citation><citation key="Proc01"><doi>10.1109/ICIMT.2009.11</doi></citation><citation key="Atr04"><doi>10.1016/j.jss.2011.12.023</doi></citation><citation key="Atr02"><doi>10.1007/s40979-023-00131-6</doi></citation></citation_list></conference_paper><conference_paper><contributors><person_name sequence="first" contributor_role="author"><given_name>Benedikt</given_name><surname>Heidrich</surname><affiliations><institution><institution_name>sktime</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0002-1923-0848</ORCID><alt-name><string-name>Benedikt Heidrich</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Sankalp</given_name><surname>Gilda</surname><affiliations><institution><institution_name>DevelopYours, LLC</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0002-3645-4501</ORCID><alt-name><string-name>Sankalp Gilda</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Franz</given_name><surname>Kiraly</surname><affiliations><institution><institution_name>sktime</institution_name></institution></affiliations><alt-name><string-name>Franz Kiraly</string-name></alt-name></person_name></contributors><titles><title>Evaluating Probabilistic Forecasters with sktime and tsbootstrap — Easy-to-Use, Configurable Frameworks for Reproducible Science</title></titles><jats:abstract><jats:p>Evaluating probabilistic forecasts is complex and essential across various domains, yet no comprehensive software framework exists to simplify this task. Despite extensive literature on evaluation methodologies, current practices are fragmented and often lack reproducibility. To address this gap, we introduce a reproducible experimental workflow for evaluating probabilistic forecasting algorithms using the sktime package. Our framework features a unified software API for forecasting algorithms, a simple specification language for complex algorithms, including meta-algorithms like bootstrapping, probabilistic performance metrics, and standardized evaluation workflows. We demonstrate the framework’s efficacy through a study evaluating prediction intervals added to point forecasts. Our results highlight the improved prediction accuracy and reliability of combined approaches. We provide reusable code and invite contributions from the research community to extend our experiments and tackle computational challenges for broader studies.</jats:p></jats:abstract><publication_date media_type="online"><month>07</month><day>10</day><year>2024</year></publication_date><pages><first_page>78</first_page><last_page>93</last_page></pages><ai:program name="AccessIndicators"><ai:free_to_read></ai:free_to_read><ai:license_ref applies_to="vor">https://creativecommons.org/licenses/by/4.0/</ai:license_ref></ai:program><doi_data><doi>10.25080/VPNX1595</doi><resource content_version="vor">https://doi.curvenote.com/10.25080/VPNX1595</resource></doi_data><citation_list><citation key="gneiting2014probabilistic"><doi>10.1146/annurev-statistics-062713-085831</doi></citation><citation key="makridakis2020_m4"><doi>10.1016/j.ijforecast.2019.04.014</doi></citation><citation key="makridakis2022_m5"><doi>10.1016/j.ijforecast.2021.11.013</doi></citation><citation key="chen2020probabilistic"><doi>10.1016/j.neucom.2020.03.011</doi></citation><citation key="nowotarski2018probabilistic"><doi>10.1016/j.rser.2017.05.234</doi></citation><citation key="lagllama"><doi>10.48550/arXiv.2310.08278</doi></citation><citation key="decoder_foundation_forecasting"><doi>10.48550/arXiv.2310.10688</doi></citation><citation key="semmelrock2023"><doi>10.48550/arXiv.2307.10320</doi></citation><citation key="kiraly2018"><doi>10.48550/arXiv.1812.07519</doi></citation><citation key="franz_kiraly_2024_11095261_sktime"><doi>10.5281/zenodo.11095261</doi></citation><citation key="autotheta"><doi>10.1016/j.ejor.2020.01.007</doi></citation><citation key="gilda_2024_10866090_tsbootstrap"><doi>10.5281/zenodo.10866090</doi></citation><citation key="gilda2024tsbootstrap"><doi>10.48550/arXiv.2404.15227</doi></citation><citation key="Godahewa2021Australian"><doi>10.5281/zenodo.4659727</doi></citation><citation key="Godahewa2021Sunspot"><doi>10.5281/zenodo.4654722</doi></citation><citation key="Godahewa2021USBirth"><doi>10.5281/zenodo.4656049</doi></citation><citation key="loning2019sktime"><doi>10.48550/arXiv.1909.07872</doi></citation><citation key="seabold2010statsmodels"><doi>10.5281/zenodo.10984387</doi></citation><citation key="pedregosa2011scikit"><doi>10.48550/arXiv.1201.0490</doi></citation><citation key="bischl2016mlr"><doi>10.48550/arXiv.1609.06146</doi></citation><citation key="hall2009weka"><doi>10.1145/1656274.1656278</doi></citation><citation key="kiraly2021designing"><doi>10.48550/arXiv.2101.04938</doi></citation><citation key="Assimakopoulos2000"><doi>10.1016/S0169-2070(00)00066-2</doi></citation><citation key="Stankeviciute2021"><doi>10.48550/arXiv.0706.3188</doi></citation><citation key="Sedgwicke509"><doi>10.1136/bmj.e509</doi></citation><citation key="bergmeir2016"><doi>10.1016/j.ijforecast.2015.07.002</doi></citation><citation key="matheson1976"><doi>10.1287/mnsc.22.10.1087</doi></citation><citation key="pinball_loss"><doi>10.48550/arXiv.1102.2101</doi></citation><citation key="phipps2024"><doi>10.1007/s10489-024-05346-9</doi></citation><citation key="wang2020"><doi>10.1016/j.epsr.2020.106732</doi></citation></citation_list></conference_paper><conference_paper><contributors><person_name sequence="first" contributor_role="author"><given_name>Blaine H. M.</given_name><surname>Mooers</surname><affiliations><institution><institution_name>Department of Biochemistry and Physiology, College of Medicine, University of Oklahoma Health Sciences, Oklahoma City, OK 73104, USA</institution_name></institution><institution><institution_name>Laboratory of Biomolecular Structure and Function, University of Oklahoma Health Sciences, Oklahoma City, OK 73104, USA</institution_name></institution><institution><institution_name>Stephenson Cancer Center, University of Oklahoma Health Sciences, Oklahoma City, OK 73104, USA</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0001-8181-8987</ORCID><alt-name><string-name>Blaine H. M. Mooers</string-name></alt-name></person_name></contributors><titles><title>Voice Computing with Python in Jupyter Notebooks</title></titles><jats:abstract><jats:p>Jupyter is a popular platform for writing interactive computational narratives that contain computer code and its output interleaved with prose that describes the code and the output. It is possible to use one’s voice to interact with Jupyter notebooks. This capability improves access to those with impaired use of their hands. Voice computing also increases the productivity of workers who are tired of typing and increases the productivity of those workers who speak faster than they can type. Voice computing can be divided into three activities: speech-to-text, speech-to-command, and speech-to-code. We will provide examples of the first two activities with the Voice-In Plus plugin for Google Chrome and Microsoft Edge. To support the editing of Markdown and code cells in Jupyter notebooks, we provide several libraries of voice commands at MooersLab on GitHub.</jats:p></jats:abstract><publication_date media_type="online"><month>07</month><day>10</day><year>2024</year></publication_date><pages><first_page>94</first_page><last_page>105</last_page></pages><ai:program name="AccessIndicators"><ai:free_to_read></ai:free_to_read><ai:license_ref applies_to="vor">https://creativecommons.org/licenses/by/4.0/</ai:license_ref></ai:program><doi_data><doi>10.25080/MCYV2126</doi><resource content_version="vor">https://doi.curvenote.com/10.25080/MCYV2126</resource></doi_data><citation_list><citation key="Mooers2021TemplatesForWritingPyMOLScripts"><doi>10.1002/pro.3997</doi></citation><citation key="Mooers2021APyMOLSnippetLibraryForJupyterToBoostResearcherProductivity"><doi>10.1109/MCSE.2021.3059536</doi></citation><citation key="seaborn"><doi>10.21105/joss.03021</doi></citation><citation key="tabibian2019enhancinghumanlearningviaspacedrepetitionoptimization"><doi>10.1073/pnas.1815156116</doi></citation></citation_list></conference_paper><conference_paper><contributors><person_name sequence="first" contributor_role="author"><given_name>Brian</given_name><surname>Falkenstein</surname><affiliations><institution><institution_name>PredxBio, Inc.</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0002-9629-1409</ORCID><alt-name><string-name>Brian Falkenstein</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Shannon</given_name><surname>Quinn</surname><affiliations><institution><institution_name>PredxBio, Inc.</institution_name></institution><institution><institution_name>School of Computer Science, Department of Cellular Biology, University of Georgia</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0002-8916-6335</ORCID><alt-name><string-name>Shannon Quinn</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Chakra</given_name><surname>Chennubhotla</surname><affiliations><institution><institution_name>PredxBio, Inc.</institution_name></institution><institution><institution_name>Dept of Computational and Systems Biology, University of Pittsburgh School of Medicine</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0002-0024-1627</ORCID><alt-name><string-name>Chakra Chennubhotla</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Filippo</given_name><surname>Pullara</surname><affiliations><institution><institution_name>PredxBio, Inc.</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0002-2283-4968</ORCID><alt-name><string-name>Filippo Pullara</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Raymond</given_name><surname>Yan</surname><affiliations><institution><institution_name>PredxBio, Inc.</institution_name></institution></affiliations><alt-name><string-name>Raymond Yan</string-name></alt-name></person_name></contributors><titles><title>Predx-Tools</title><subtitle>Dispelling the Mystery in Histopathological Image Processing</subtitle></titles><jats:abstract><jats:p>Histopathological images, which are digitized images of human or animal tissue, contain insights into disease state. Typically, a pathologist will look at a slide under a microscope to make decisions about prognosis and treatment. Due to the high complexity of the data, applying automatic image analysis is challenging. Often, human intervention in the form of manual annotation or quality control (<jats:abbrev alt="Quality control">QC</jats:abbrev>) is required. Additionally, the data itself varies considerably in available features, size, and shape. Thus, a streamlined and interactive approach is a necessary part of any digital pathology pipeline. We present PredX-Tools, a suite of simple and easy to use python <jats:abbrev alt="Graphical user interface">GUI</jats:abbrev> applications which facilitate analysis of histopathological images and provide a no-code platform for data scientists and researchers to perform analysis on raw and transformed data.</jats:p></jats:abstract><publication_date media_type="online"><month>07</month><day>10</day><year>2024</year></publication_date><pages><first_page>106</first_page><last_page>113</last_page></pages><ai:program name="AccessIndicators"><ai:free_to_read></ai:free_to_read><ai:license_ref applies_to="vor">https://creativecommons.org/licenses/by/4.0/</ai:license_ref></ai:program><doi_data><doi>10.25080/YCFW5807</doi><resource content_version="vor">https://doi.curvenote.com/10.25080/YCFW5807</resource></doi_data><citation_list><citation key="Nam_2020"><doi>10.4132/jptm.2019.12.31</doi></citation><citation key="Rojas_2022"><doi>10.3389/fonc.2022.889886</doi></citation><citation key="Kaushal_2019"><doi>10.1016/j.irbm.2019.06.001</doi></citation><citation key="Gray_2023"><doi>10.3390/cancers15194797</doi></citation><citation key="Baxi_2022"><doi>10.1038/s41379-021-00919-2</doi></citation><citation key="Wilson_2021"><doi>10.3390/cancers13123031</doi></citation><citation key="Mulholland_2024"><doi>10.1308/rcsann.2023.0091</doi></citation><citation key="Bod_n_2021"><doi>10.1111/his.14356</doi></citation><citation key="matplotlib"><doi>10.1109/MCSE.2007.55</doi></citation><citation key="numpy"><doi>10.1038/s41586-020-2649-2</doi></citation><citation key="pandas1"><doi>10.5281/zenodo.3509134</doi></citation><citation key="pandas2"><doi>10.25080/Majora-92bf1922-00a</doi></citation><citation key="scipy"><doi>10.1038/s41592-019-0686-2</doi></citation><citation key="fiji"><doi>10.1038/nmeth.2089</doi></citation><citation key="qupath"><doi>10.1038/s41598-017-17204-5</doi></citation><citation key="cellprof"><doi>10.1186/s12859-021-04344-9</doi></citation><citation key="ilastik"><doi>10.1038/s41592-019-0582-9</doi></citation><citation key="Lin_2023"><doi>10.1038/s43018-023-00576-1</doi></citation><citation key="McKinley_2017"><doi>10.1172/jci.insight.93487</doi></citation><citation key="Ben_Said_2021"><doi>10.1186/s13717-021-00314-4</doi></citation><citation key="Franklin_2008"><doi>10.1007/978-3-642-01976-0_9</doi></citation><citation key="Ripley_1977"><doi>10.1111/j.2517-6161.1977.tb01615.x</doi></citation><citation key="Church_1989"><doi>10.3115/981623.981633</doi></citation><citation key="Langseth_2021"><doi>10.1038/s42003-021-02517-z</doi></citation></citation_list></conference_paper><conference_paper><contributors><person_name sequence="first" contributor_role="author"><given_name>Christopher</given_name><surname>Ariza</surname><affiliations><institution><institution_name>Research Affiliates</institution_name></institution></affiliations><ORCID>https://orcid.org/0009-0000-8549-0210</ORCID><alt-name><string-name>Christopher Ariza</string-name></alt-name></person_name></contributors><titles><title>Improving Code Quality with Array and DataFrame Type Hints</title></titles><jats:abstract><jats:p>This article demonstrates practical approaches to fully type-hinting generic NumPy arrays and StaticFrame DataFrames, and shows how the same annotations can improve code quality with both static analysis and runtime validation.</jats:p></jats:abstract><publication_date media_type="online"><month>07</month><day>10</day><year>2024</year></publication_date><pages><first_page>114</first_page><last_page>120</last_page></pages><ai:program name="AccessIndicators"><ai:free_to_read></ai:free_to_read><ai:license_ref applies_to="vor">https://creativecommons.org/licenses/by/4.0/</ai:license_ref></ai:program><doi_data><doi>10.25080/WPXM6451</doi><resource content_version="vor">https://doi.curvenote.com/10.25080/WPXM6451</resource></doi_data><citation_list><citation key="numpy"><doi>10.1038/s41586-020-2649-2</doi></citation><citation key="pandera"><doi>10.25080/Majora-342d178e-010</doi></citation></citation_list></conference_paper><conference_paper><contributors><person_name sequence="first" contributor_role="author"><given_name>Rowan</given_name><surname>Cockett</surname><affiliations><institution><institution_name>Curvenote Inc.</institution_name><institution_id type="ror">https://ror.org/02mz0e468</institution_id><institution_place>Canada</institution_place></institution><institution><institution_name>Project Jupyter</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0002-7859-8394</ORCID><alt-name><string-name>Rowan Cockett</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Steve</given_name><surname>Purves</surname><affiliations><institution><institution_name>Curvenote Inc.</institution_name><institution_id type="ror">https://ror.org/02mz0e468</institution_id><institution_place>Canada</institution_place></institution><institution><institution_name>Project Jupyter</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0002-0760-5497</ORCID><alt-name><string-name>Steve Purves</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Franklin</given_name><surname>Koch</surname><affiliations><institution><institution_name>Curvenote Inc.</institution_name><institution_id type="ror">https://ror.org/02mz0e468</institution_id><institution_place>Canada</institution_place></institution><institution><institution_name>Project Jupyter</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0002-6393-7058</ORCID><alt-name><string-name>Franklin Koch</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Mike</given_name><surname>Morrison</surname><affiliations><institution><institution_name>Curvenote Inc.</institution_name><institution_id type="ror">https://ror.org/02mz0e468</institution_id><institution_place>Canada</institution_place></institution></affiliations><ORCID>https://orcid.org/0000-0002-1324-8801</ORCID><alt-name><string-name>Mike Morrison</string-name></alt-name></person_name></contributors><titles><title>Continuous Tools for Scientific Publishing</title><subtitle>Using MyST Markdown and Curvenote to encourage continuous science practices</subtitle></titles><jats:abstract><jats:p>Advances in technology for data workflows have increased the speed and scope of scientific discovery, however, scientific dialogue still uses outdated technology for communicating and sharing knowledge.
The widespread reliance on static <jats:abbrev alt="Portable Document Format">PDF</jats:abbrev> formats for research papers starkly contrasts with the complex, data-driven and increasingly computational nature of modern science.
This gap, which is especially evident in the computational sciences, impedes the speed of research dissemination, reuse, and uptake.
We require new mediums to compose ideas and ways to share research findings iteratively, as early as possible and connected <jats:italic>directly</jats:italic> to software and data.
In this paper we discuss two tools for scientific authoring and publishing, <jats:abbrev alt="Markedly Structured Text">MyST</jats:abbrev> Markdown and Curvenote, and illustrate examples of improving metadata, reimagining the reading experience, including computational content, and transforming publishing practices for individuals and societies through automation and continuous practices.
We focus on the unique aspects of the tools, which enable computational and interactive content, publishing and sharing continuously through automated checking and typesetting, and provide case studies from individuals to societies who have adopted these tools.</jats:p></jats:abstract><publication_date media_type="online"><month>07</month><day>10</day><year>2024</year></publication_date><pages><first_page>121</first_page><last_page>136</last_page></pages><ai:program name="AccessIndicators"><ai:free_to_read></ai:free_to_read><ai:license_ref applies_to="vor">https://creativecommons.org/licenses/by/4.0/</ai:license_ref></ai:program><doi_data><doi>10.25080/NKVC9349</doi><resource content_version="vor">https://doi.curvenote.com/10.25080/NKVC9349</resource></doi_data><citation_list><citation key="2019"><doi>10.20316/ese.2019.45.18018</doi></citation><citation key="https://doi.org/10.4230/dagman.1.1.41"><doi>10.4230/DAGMAN.1.1.41</doi></citation><citation key="Grossmann_2021"><doi>10.12688/f1000research.27468.2</doi></citation><citation key="Hagve_2020"><doi>10.4045/tidsskr.20.0118</doi></citation><citation key="Clotworthy_2023"><doi>10.1186/s12916-023-02882-y</doi></citation><citation key="Xie_2018"><doi>10.1201/9781138359444</doi></citation><citation key="binder"><doi>10.25080/majora-4af1f417-011</doi></citation><citation key="Caprarelli_2023"><doi>10.1029/2023ea003458</doi></citation><citation key="Heidt_2024"><doi>10.1038/d41586-024-02577-1</doi></citation><citation key="Mishra_2020"><doi>10.1016/j.cosrev.2020.100308</doi></citation><citation key="Leite_2019"><doi>10.1145/3359981</doi></citation><citation key="Chen_2015"><doi>10.1109/ms.2015.27</doi></citation><citation key="Callanan_2016"><doi>10.1109/ms.2016.66</doi></citation><citation key="Powell2016Does"><doi>10.1038/530148a</doi></citation><citation key="Jones_2016"><doi>10.7771/2380-176x.7269</doi></citation><citation key="https://doi.org/10.17863/cam.96819"><doi>10.17863/CAM.96819</doi></citation><citation key="Clark_2014"><doi>10.1186/2041-1480-5-28</doi></citation><citation key="Groth_2010"><doi>10.3233/isu-2010-0613</doi></citation><citation key="Teytelman_2016"><doi>10.1371/journal.pbio.1002538</doi></citation><citation key="Nosek_2018"><doi>10.1073/pnas.1708274114</doi></citation><citation key="Smith_2018"><doi>10.7717/peerj-cs.147</doi></citation><citation key="Lenharo_2024"><doi>10.1038/d41586-024-00996-8</doi></citation><citation key="Penfold_2020"><doi>10.1371/journal.pgen.1008565</doi></citation><citation key="Johansson_2020"><doi>10.1038/d41586-020-00613-4</doi></citation><citation key="https://doi.org/10.5281/zenodo.6476040"><doi>10.5281/ZENODO.6476040</doi></citation><citation key="Head_2021"><doi>10.1145/3411764.3445648</doi></citation><citation key="Shneiderman"><doi>10.1109/vl.1996.545307</doi></citation><citation key="Barnett_2020"><doi>10.7554/elife.60080</doi></citation><citation key="Dunn_2003"><doi>10.1109/mitp.2003.1176491</doi></citation><citation key="Cockett_2016"><doi>10.1190/tle35080703.1</doi></citation><citation key="Link_2008"><doi>10.1016/j.econedurev.2007.04.002</doi></citation><citation key="Evans_2018"><doi>10.1038/nbt.4089</doi></citation><citation key="Watson_2022"><doi>10.1038/s41591-021-01654-6</doi></citation></citation_list></conference_paper><conference_paper><contributors><person_name sequence="first" contributor_role="author"><given_name>Elijah</given_name><surname>Knaap</surname><affiliations><institution><institution_name>San Diego State University</institution_name></institution><institution><institution_name>Center for Open Geographical Science</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0001-7520-2238</ORCID><alt-name><string-name>Elijah Knaap</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Sergio</given_name><surname>Rey</surname><affiliations><institution><institution_name>San Diego State University</institution_name></institution><institution><institution_name>Center for Open Geographical Science</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0001-5857-9762</ORCID><alt-name><string-name>Sergio Rey</string-name></alt-name></person_name></contributors><titles><title>geosnap: The Geospatial Neighborhood Analysis Package</title><subtitle>Open Tools for Urban, Regional, and Neighborhood Science</subtitle></titles><jats:abstract><jats:p>Understanding neighborhood context is critical for social science research, public
policy analysis, and urban planning. The social meaning, formal definition, and formal
operationalization of “neighborhood” depends on the study or application, however, so
neighborhood analysis and modeling requires both flexibility and adherence to a formal
pipeline. Maintaining that balance is challenging for a variety of reasons. To address
those challenges, <jats:monospace>geosnap</jats:monospace>, the Geospatial Neighborhood Analysis Package provides a
suite of tools for exploring, modeling, and visualizing the social context and spatial
extent of neighborhoods and regions over time. It brings together state-of-the-art
techniques from geodemographics, regionalization, spatial data science, and
segregation analysis to support social science research, public policy analysis, and
urban planning. It provides a simple interface tailored to formal analysis of
spatiotemporal urban data.</jats:p></jats:abstract><publication_date media_type="online"><month>07</month><day>10</day><year>2024</year></publication_date><pages><first_page>137</first_page><last_page>153</last_page></pages><ai:program name="AccessIndicators"><ai:free_to_read></ai:free_to_read><ai:license_ref applies_to="vor">https://creativecommons.org/licenses/by/4.0/</ai:license_ref></ai:program><doi_data><doi>10.25080/FVWM4182</doi><resource content_version="vor">https://doi.curvenote.com/10.25080/FVWM4182</resource></doi_data><citation_list><citation key="cortes2020OpensourceFramework"><doi>10.1007/s42001-019-00059-3</doi></citation><citation key="finio2022SmartGrowth"><doi>10.4337/9781789904697.00026</doi></citation><citation key="Kang2021"><doi>10.1177/00420980211042549</doi></citation><citation key="Knaap2017"><doi>10.1080/10511482.2017.1331930</doi></citation><citation key="knaap2023SegregatedDesign"><doi>10.1177/23998083231197956</doi></citation><citation key="Rey2022a"><doi>10.1177/01600176221116566</doi></citation><citation key="wei2022ReducingRacial"><doi>10.1016/j.seps.2022.101415</doi></citation><citation key="Logan_2014"><doi>10.1080/00330124.2014.905156</doi></citation><citation key="Agovino2019"><doi>10.1016/j.socscimed.2018.11.013</doi></citation><citation key="gallo_space-time_2004-1"><doi>10.1177/0160017603262402</doi></citation><citation key="kang_rey_2018_ars"><doi>10.1007/s00168-017-0859-9</doi></citation><citation key="rey_2016_jgs"><doi>10.1007/s10109-016-0234-x</doi></citation><citation key="Rey2014"><doi>10.1007/s10109-013-0189-0</doi></citation><citation key="Lee2017"><doi>10.1080/07352166.2016.1251154</doi></citation><citation key="Abbas2009"><doi>10.1016/j.puhe.2008.10.007</doi></citation><citation key="Adnan2010"><doi>10.1111/j.1467-9671.2010.01197.x</doi></citation><citation key="anderson2010UsingGeodemographics"><doi>10.1068/a43157</doi></citation><citation key="singleton2009CreatingOpen"><doi>10.1111/j.1435-5957.2008.00197.x</doi></citation><citation key="singleton2009GeodemographicsVisualisation"><doi>10.1016/j.apgeog.2008.10.006</doi></citation><citation key="Berry1971"><doi>10.2307/143204</doi></citation><citation key="Hunter1972"><doi>10.2307/2060548</doi></citation><citation key="lebowitz1977CriticalExamination"><doi>10.2307/40022220</doi></citation><citation key="Perle1979"><doi>10.1111/j.1538-4632.1979.tb00708.x</doi></citation><citation key="Rees1969"><doi>10.1086/224681</doi></citation><citation key="Bell1962"><doi>10.2307/1388270</doi></citation><citation key="Brindley1979"><doi>10.1080/713702552</doi></citation><citation key="Spielman2008"><doi>10.1016/j.compenvurbsys.2007.11.004</doi></citation><citation key="Park1936"><doi>10.2307/2084475</doi></citation><citation key="Park1952"><doi>10.2307/2087814</doi></citation><citation key="Calafiore2022"><doi>10.1016/j.trd.2021.103111</doi></citation><citation key="hipp2013EGOHOODSWAVES"><doi>10.1111/1745-9125.12006</doi></citation><citation key="Kim2019"><doi>10.1007/s10940-019-09410-3</doi></citation><citation key="geisberger2012ExactRouting"><doi>10.1287/trsc.1110.0401</doi></citation></citation_list></conference_paper><conference_paper><contributors><person_name sequence="first" contributor_role="author"><given_name>Emily</given_name><surname>Dorne</surname><affiliations><institution><institution_name>DrivenData</institution_name></institution></affiliations><ORCID>https://orcid.org/0009-0004-6185-7611</ORCID><alt-name><string-name>Emily Dorne</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Katie</given_name><surname>Wetstone</surname><affiliations><institution><institution_name>DrivenData</institution_name></institution></affiliations><ORCID>https://orcid.org/0009-0003-3338-7600</ORCID><alt-name><string-name>Katie Wetstone</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Trista Brophy</given_name><surname>Cerquera</surname><affiliations><institution><institution_name>NASA</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0002-4852-6250</ORCID><alt-name><string-name>Trista Brophy Cerquera</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Shobhana</given_name><surname>Gupta</surname><affiliations><institution><institution_name>NASA</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0002-8896-1353</ORCID><alt-name><string-name>Shobhana Gupta</string-name></alt-name></person_name></contributors><titles><title>Cyanobacteria detection in small, inland water bodies with CyFi</title><subtitle>Using satellite imagery and machine learning to protect public health</subtitle></titles><jats:abstract><jats:p>Harmful algal blooms (<jats:abbrev alt="harmful algal bloom">HAB</jats:abbrev>s) pose major health risks to human and aquatic life. Remote sensing-based methods exist to automatically detect large, slow-moving <jats:abbrev alt="harmful algal bloom">HAB</jats:abbrev>s in the ocean, but fall short for smaller, more dynamic blooms in critical inland water bodies like lakes, reservoirs, and rivers.</jats:p><jats:p><jats:abbrev alt="Cyanobacteria Finder">CyFi</jats:abbrev> is an open-source Python package that enables detection of cyanobacteria in inland water bodies using 10-30m Sentinel-2 imagery and a computationally efficient tree-based machine learning model. <jats:abbrev alt="Cyanobacteria Finder">CyFi</jats:abbrev> enables water quality and public health managers to conduct high level assessments of water bodies of interest and identify regions in which to target monitoring and responsive actions to protect public health.</jats:p><jats:p><jats:abbrev alt="Cyanobacteria Finder">CyFi</jats:abbrev> was developed in three phases. A machine learning competition leveraged the diverse skills and creativity of data science experts to surface promising approaches for cyanobacteria detection from remote sensed data. Subsequent user interviews and model iteration resulted in a deployment-ready open-source package designed to meet user workflow needs and decision-making priorities. This process illustrates a replicable pathway for developing powerful machine learning tools in domain-specific areas.</jats:p></jats:abstract><publication_date media_type="online"><month>07</month><day>10</day><year>2024</year></publication_date><pages><first_page>154</first_page><last_page>173</last_page></pages><ai:program name="AccessIndicators"><ai:free_to_read></ai:free_to_read><ai:license_ref applies_to="vor">https://creativecommons.org/licenses/by/4.0/</ai:license_ref></ai:program><doi_data><doi>10.25080/PDHK7238</doi><resource content_version="vor">https://doi.curvenote.com/10.25080/PDHK7238</resource></doi_data><citation_list><citation key="Papenfus_2020"><doi>10.1007/s10661-020-08631-5</doi></citation><citation key="Pulido_2016"><doi>10.15406/icpjl.2016.02.00062</doi></citation><citation key="Treuer_2021"><doi>10.1038/s41893-021-00770-y</doi></citation><citation key="Lad_2022"><doi>10.3390/life12030418</doi></citation><citation key="Watson_2016"><doi>10.1016/j.hal.2016.04.010</doi></citation><citation key="Hoagland_2009"><doi>10.1289/ehp.0900645</doi></citation><citation key="Hoagland_2002"><doi>10.1007/bf02804908</doi></citation><citation key="Khan_2021"><doi>10.3390/rs13214347</doi></citation><citation key="Clark_2017"><doi>10.1016/j.ecolind.2017.04.046</doi></citation><citation key="https://doi.org/10.48550/arxiv.1606.07781"><doi>10.48550/ARXIV.1606.07781</doi></citation><citation key="Da_Poian_2024"><doi>10.1093/rasti/rzae009</doi></citation><citation key="Johnson_2021"><doi>10.1073/pnas.2011362118</doi></citation><citation key="seabass"><doi>10.5067/SeaBASS/CAML/DATA001</doi></citation><citation key="https://doi.org/10.48550/arxiv.1603.02754"><doi>10.48550/ARXIV.1603.02754</doi></citation><citation key="https://doi.org/10.48550/arxiv.1810.11363"><doi>10.48550/ARXIV.1810.11363</doi></citation><citation key="Urquhart_2017"><doi>10.1016/j.hal.2017.06.001</doi></citation><citation key="Wynne_2010"><doi>10.4319/lo.2010.55.5.2025</doi></citation><citation key="Mishra_2019"><doi>10.1038/s41598-019-54453-y</doi></citation><citation key="Wynne_2008"><doi>10.1080/01431160802007640</doi></citation><citation key="Mishra_2021"><doi>10.1016/j.scitotenv.2021.145462</doi></citation></citation_list></conference_paper><conference_paper><contributors><person_name sequence="first" contributor_role="author"><given_name>Forrest Sheng</given_name><surname>Bao</surname><affiliations><institution><institution_name>Founder, Textea, Inc.</institution_name></institution></affiliations><alt-name><string-name>Forrest Sheng Bao</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Mike</given_name><surname>Qi</surname><affiliations><institution><institution_name>Engineer, Textea, Inc.</institution_name></institution></affiliations><alt-name><string-name>Mike Qi</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Ruixuan</given_name><surname>Tu</surname><affiliations><institution><institution_name>Undergraduate student, Dept. of Computer Sciences, University of Wisconsin-Madison</institution_name></institution></affiliations><alt-name><string-name>Ruixuan Tu</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Erana</given_name><surname>Wan</surname><affiliations><institution><institution_name>MS student, Dept. of Computer Science, University of Southern California</institution_name></institution></affiliations><alt-name><string-name>Erana Wan</string-name></alt-name></person_name></contributors><titles><title>Funix - The laziest way to build GUI apps in Python</title><subtitle>Make it (the app) before they fake it (in Figma or on a napkin)</subtitle></titles><jats:abstract><jats:p>The rise of machine learning (<jats:abbrev alt="machine learning">ML</jats:abbrev>) and artificial intelligence (<jats:abbrev alt="artificial intelligence">AI</jats:abbrev>), especially the generative <jats:abbrev alt="artificial intelligence">AI</jats:abbrev> (<jats:abbrev alt="generative artificial intelligence">GenAI</jats:abbrev>), has increased the need for wrapping models or algorithms into <jats:abbrev alt="graphical user interface">GUI</jats:abbrev> apps. For example, a large language model (<jats:abbrev alt="large language model">LLM</jats:abbrev>) can be accessed through a string-to-string <jats:abbrev alt="graphical user interface">GUI</jats:abbrev> app with a textbox as the primary input.
Most of existing solutions require developers to manually create widgets and link them to arguments/returns of a function individually. This low-level process is laborious and usually intrusive. Funix automatically selects widgets based on the types of the arguments and returns of a function according to the type-to-widget mapping defined in a theme, e.g., <jats:monospace>bool</jats:monospace> to a checkbox. Consequently, an existing Python function can be turned into a <jats:abbrev alt="graphical user interface">GUI</jats:abbrev> app without any code changes. As a transcompiler, Funix allows type-to-widget mappings to be defined between any Python type and any React component and its <jats:monospace>props</jats:monospace>, liberating Python developers to the frontend world without needing to know JavaScript/TypeScript. Funix further leverages features in Python or its ecosystem for building apps in a more Pythonic, intuitive, and effortless manner. With Funix, a developer can make it (a functional app) before they (competitors) fake it (in Figma or on a napkin).</jats:p></jats:abstract><publication_date media_type="online"><month>07</month><day>10</day><year>2024</year></publication_date><pages><first_page>174</first_page><last_page>195</last_page></pages><ai:program name="AccessIndicators"><ai:free_to_read></ai:free_to_read><ai:license_ref applies_to="vor">https://creativecommons.org/licenses/by/4.0/</ai:license_ref></ai:program><doi_data><doi>10.25080/JFYN3740</doi><resource content_version="vor">https://doi.curvenote.com/10.25080/JFYN3740</resource></doi_data></conference_paper><conference_paper><contributors><person_name sequence="first" contributor_role="author"><given_name>Justin</given_name><surname>Gagnon</surname><affiliations><institution><institution_name>Department of Biology, University of Sherbrooke, 2500, boul. de l'Université, Sherbrooke, Quebec, J1K 2R1 Canada</institution_name></institution></affiliations><alt-name><string-name>Justin Gagnon</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Nadia</given_name><surname>Tahiri</surname><affiliations><institution><institution_name>Department of Computer Science, University of Sherbrooke, 2500, boul. de l'Université, Sherbrooke, Quebec, J1K 2R1 Canada</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0002-1818-208X</ORCID><alt-name><string-name>Nadia Tahiri</string-name></alt-name></person_name></contributors><titles><title>Ecological and Spatial Influences on the Genetics of Cumacea (Crustacea: Peracarida) in the Northern North Atlantic</title><subtitle>by aPhyloGeo software</subtitle></titles><jats:abstract><jats:p>The peracarid taxon Cumacea is an essential indicator of benthic quality in marine ecosystems. This study investigated the influence of environmental (i.e., biological or ecosystemic), climatic (i.e., meteorological or atmospheric), and spatial (i.e., geographic or regional) variables on their genetic variability and adaptability in the Northern North Atlantic, focusing on Icelandic waters. We analyzed partial sequences of the 16S <jats:abbrev alt="Ribosomal ribonucleic acid">rRNA</jats:abbrev> mitochondrial gene from 62 Cumacea specimens. Using the <jats:italic>aPhyloGeo</jats:italic> software, we compared these sequences with relevant variables such as latitude (decimal degree) at the end of sampling, wind speed (m/s) at the start of sampling, O<jats:sub>2</jats:sub> concentration (mg/L), and depth (m) at the start of sampling.</jats:p><jats:p>Our analyses revealed variability in spatial and biological variables, reflecting the diversity of ecological requirements and benthic habitats. The most common Cumacea families, Diastylidae and Leuconidae, suggest adaptations to various marine environments. Phylogeographic analysis showed a divergence between specific genetic sequences and two habitat variables: wind speed (m/s) at the start of sampling and O<jats:sub>2</jats:sub> concentration (mg/L). This observation may indicate the possibility of varying local adaptations in response to these fluctuating conditions.</jats:p><jats:p>These results reinforce the importance of further research into the relationship between Cumacea genetics and global environmental variables to interpret the evolutionary dynamics and adaptation of these deep-sea organisms. This study sheds much-needed light on the acclimatization of invertebrates to climate change, anthropogenic pressures, and marine habitat management, potentially contributing to the evolution of more effective conservation strategies and policies to protect these vulnerable ecosystems.</jats:p><jats:p>The <jats:italic>aPhyloGeo</jats:italic> Python package is freely and publicly available on <jats:ext-link ext-link-type="uri" xlink:href="https://github.com/tahiri-lab/aPhyloGeo">GitHub</jats:ext-link> and <jats:ext-link ext-link-type="uri" xlink:href="https://pypi.org/project/aphylogeo/">PyPi</jats:ext-link>, providing an invaluable tool for future research.</jats:p></jats:abstract><publication_date media_type="online"><month>07</month><day>10</day><year>2024</year></publication_date><pages><first_page>196</first_page><last_page>215</last_page></pages><ai:program name="AccessIndicators"><ai:free_to_read></ai:free_to_read><ai:license_ref applies_to="vor">https://creativecommons.org/licenses/by/4.0/</ai:license_ref></ai:program><doi_data><doi>10.25080/NVYF1037</doi><resource content_version="vor">https://doi.curvenote.com/10.25080/NVYF1037</resource></doi_data><citation_list><citation key="schnurr_composition_2014"><doi>10.1016/j.dsr.2013.11.004</doi></citation><citation key="meisner_benthic_2014"><doi>10.2478/popore-2014-0016</doi></citation><citation key="uhlir_adding_2021"><doi>10.7717/peerj.12379</doi></citation><citation key="levin2009ecological"><doi>10.1016/j.tree.2009.04.012</doi></citation><citation key="rogers2007corals"><doi>10.1002/9780470691953.ch8</doi></citation><citation key="danovaro2008exponential"><doi>10.1016/j.cub.2007.11.056</doi></citation><citation key="meisner_prefacebiodiversity_2018"><doi>10.1007/s12526-018-0884-7</doi></citation><citation key="stransky_diversity_2010"><doi>10.1007/s00300-009-0691-5</doi></citation><citation key="rehm2009cumacea"><doi>10.2312/BzPM_0602_2009</doi></citation><citation key="jennings_phylogeographic_2014"><doi>10.2478/popore-2014-0017</doi></citation><citation key="grassle1992deep"><doi>10.1086/285329</doi></citation><citation key="rex1997large"><doi>10.1017/CBO9780511752360.006</doi></citation><citation key="brown2011benthic"><doi>10.1016/j.ecss.2011.02.007</doi></citation><citation key="vrijenhoek2009cryptic"><doi>10.1016/j.dsr2.2009.05.016</doi></citation><citation key="balkenhol_identifying_2009"><doi>10.1007/s10980-009-9334-z</doi></citation><citation key="manel_perspectives_2010"><doi>10.1111/j.1365-294X.2010.04717.x</doi></citation><citation key="balkenhol_landscape_2019"><doi>10.1007/13836_2017_2</doi></citation><citation key="shafer_widespread_2013"><doi>10.1111/ele.12120</doi></citation><citation key="rex2000latitudinal"><doi>10.1073/pnas.050589497</doi></citation><citation key="etter1990population"><doi>10.1016/0198-0149(90)90041-S</doi></citation><citation key="manel2003landscape"><doi>10.1016/S0169-5347(03)00008-9</doi></citation><citation key="balkenhol2009statistical"><doi>10.1111/j.1600-0587.2009.05807.x</doi></citation><citation key="gaither2013origins"><doi>10.1111/jbi.12126</doi></citation><citation key="rex2006global"><doi>10.3354/meps317001</doi></citation><citation key="danovaro2010first"><doi>10.1186/1741-7007-8-30</doi></citation><citation key="siedlecki2016experiments"><doi>10.1038/srep27203</doi></citation><citation key="waga_recent_2020"><doi>10.1007/s00300-020-02632-3</doi></citation><citation key="saeedi_environmental_2022"><doi>10.3389/fmars.2022.804019</doi></citation><citation key="hugenholtz1998impact"><doi>10.1128/jb.180.18.4765-4774.1998</doi></citation><citation key="saccone1999evolutionary"><doi>10.1016/s0378-1119(99)00270-x</doi></citation><citation key="li2024host"><doi>10.3390/v16071133</doi></citation><citation key="li2023aphylogeo"><doi>10.25080/gerudo-f2bc6f59-00f</doi></citation><citation key="koshkarov_phylogeography_2022"><doi>10.25080/majora-212e5952-018</doi></citation><citation key="robinson_comparison_1981"><doi>10.1016/0025-5564(81)90043-2</doi></citation><citation key="li2024comparison"><doi>10.1002/ece3.70054</doi></citation><citation key="tahiri2018new"><doi>10.1186/s12862-018-1163-8</doi></citation><citation key="czarna2006topology"><doi>10.1186/1471-2148-6-105</doi></citation><citation key="smith2019bayesian"><doi>10.1098/rsbl.2018.0632</doi></citation><citation key="smith2020information"><doi>10.1093/bioinformatics/btaa614</doi></citation></citation_list></conference_paper><conference_paper><contributors><person_name sequence="first" contributor_role="author"><given_name>Heinrich</given_name><surname>Peters</surname><affiliations><institution><institution_name>Columbia University</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0002-0571-6388</ORCID><alt-name><string-name>Heinrich Peters</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Michael</given_name><surname>Parrott</surname><affiliations><institution><institution_name>Columbia University</institution_name></institution></affiliations><alt-name><string-name>Michael Parrott</string-name></alt-name></person_name></contributors><titles><title>Model Share AI</title><subtitle>An Integrated Toolkit for Collaborative Machine Learning Model Development, Provenance Tracking, and Deployment in Python</subtitle></titles><jats:abstract><jats:p>Machine learning (<jats:abbrev alt="Machine Learning">ML</jats:abbrev>) is revolutionizing a wide range of research areas and industries, but many <jats:abbrev alt="Machine Learning">ML</jats:abbrev> projects never progress past the proof-of-concept stage. To address this problem, we introduce Model Share <jats:abbrev alt="Artificial Intelligence">AI</jats:abbrev> (<jats:abbrev alt="Model Share AI">AIMS</jats:abbrev>), a platform designed to streamline collaborative model development, model provenance tracking, and model deployment, as well as a host of other functions aiming to maximize the real-world impact of <jats:abbrev alt="Machine Learning">ML</jats:abbrev> research. <jats:abbrev alt="Model Share AI">AIMS</jats:abbrev> features collaborative project spaces and a standardized model evaluation process that ranks model submissions based on their performance on holdout evaluation data, enabling users to run experiments and competitions. In addition, various model metadata are automatically captured to facilitate provenance tracking and allow users to learn from and build on previous submissions. Furthermore, <jats:abbrev alt="Model Share AI">AIMS</jats:abbrev> allows users to deploy <jats:abbrev alt="Machine Learning">ML</jats:abbrev> models built in Scikit-Learn, TensorFlow Keras, or PyTorch into live REST APIs and automatically generated web apps with minimal code. The ability to collaboratively develop and rapidly deploy models, making them accessible to non-technical end-users through automatically generated web apps, ensures that <jats:abbrev alt="Machine Learning">ML</jats:abbrev> projects can transition smoothly from concept to real-world application.</jats:p></jats:abstract><publication_date media_type="online"><month>07</month><day>10</day><year>2024</year></publication_date><pages><first_page>216</first_page><last_page>224</last_page></pages><ai:program name="AccessIndicators"><ai:free_to_read></ai:free_to_read><ai:license_ref applies_to="vor">https://creativecommons.org/licenses/by/4.0/</ai:license_ref></ai:program><doi_data><doi>10.25080/MDCE8355</doi><resource content_version="vor">https://doi.curvenote.com/10.25080/MDCE8355</resource></doi_data><citation_list><citation key="chen_developments_2020"><doi>10.1145/3399579.3399867</doi></citation><citation key="vanschoren_openml_2014"><doi>10.1145/2641190.2641198</doi></citation><citation key="van_rijn_openml_2013"><doi>10.1007/978-3-642-40994-3_46</doi></citation><citation key="olston_tensorflow-serving_2017"><doi>10.48550/arXiv.1712.06139</doi></citation><citation key="abadi_tensorflow_2016"><doi>10.48550/arXiv.1605.08695</doi></citation><citation key="paszke_pytorch_2019"><doi>10.48550/arXiv.1912.01703</doi></citation><citation key="hutter_automated_2019"><doi>10.1007/978-3-030-05318-5</doi></citation><citation key="he_automl_2021"><doi>10.1016/j.knosys.2020.106622</doi></citation><citation key="hospedales_meta-learning_2020"><doi>10.48550/arXiv.2004.05439</doi></citation><citation key="finn_model-agnostic_2017"><doi>10.48550/arXiv.1703.03400</doi></citation></citation_list></conference_paper><conference_paper><contributors><person_name sequence="first" contributor_role="author"><given_name>Henry</given_name><surname>Schreiner</surname><affiliations><institution><institution_name>Princeton University</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0002-7833-783X</ORCID><alt-name><string-name>Henry Schreiner</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Jean-Christophe</given_name><surname>Fillion-Robin</surname><affiliations><institution><institution_name>Kitware Inc.</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0002-9688-8950</ORCID><alt-name><string-name>Jean-Christophe Fillion-Robin</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Matt</given_name><surname>McCormick</surname><affiliations><institution><institution_name>Kitware Inc.</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0001-9475-3756</ORCID><alt-name><string-name>Matt McCormick</string-name></alt-name></person_name></contributors><titles><title>Scikit-build-core</title><subtitle>A modern build-backend for CPython C/C++/Fortran/Cython extensions</subtitle></titles><jats:abstract><jats:p>Discover how scikit-build-core revolutionizes Python extension building with its seamless integration of CMake and Python packaging standards. Learn about its enhanced features for cross-compilation, multi-platform support, and simplified configuration, which enable writing binary extensions with pybind11, Nanobind, Fortran, Cython, C++, and more. Dive into the transition from the classic scikit-build to the robust scikit-build-core and explore its potential to streamline package distribution across various environments.</jats:p></jats:abstract><publication_date media_type="online"><month>07</month><day>10</day><year>2024</year></publication_date><pages><first_page>225</first_page><last_page>235</last_page></pages><ai:program name="AccessIndicators"><ai:free_to_read></ai:free_to_read><ai:license_ref applies_to="vor">https://creativecommons.org/licenses/by/4.0/</ai:license_ref></ai:program><doi_data><doi>10.25080/FMKR8387</doi><resource content_version="vor">https://doi.curvenote.com/10.25080/FMKR8387</resource></doi_data><citation_list><citation key="scipy"><doi>10.1038/s41592-019-0686-2</doi></citation><citation key="scikit-build"><doi>10.5281/zenodo.2565368</doi></citation></citation_list></conference_paper><conference_paper><contributors><person_name sequence="first" contributor_role="author"><given_name>Josh</given_name><surname>Borrow</surname><affiliations><institution><institution_name>Department of Physics and Astronomy, University of Pennsylvania, 209 South 33rd Street, Philadelphia, PA, USA 19104</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0002-1327-1921</ORCID><alt-name><string-name>Josh Borrow</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Paul La</given_name><surname>Plante</surname><affiliations><institution><institution_name>Department of Computer Science, University of Nevada, Las Vegas, NV 89154</institution_name></institution><institution><institution_name>Nevada Center for Astrophysics, University of Nevada, Las Vegas, NV 89154</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0002-4693-0102</ORCID><alt-name><string-name>Paul La Plante</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>James</given_name><surname>Aguirre</surname><affiliations><institution><institution_name>Department of Physics and Astronomy, University of Pennsylvania, 209 South 33rd Street, Philadelphia, PA, USA 19104</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0002-4810-666X</ORCID><alt-name><string-name>James Aguirre</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Peter K. G.</given_name><surname>Williams</surname><affiliations><institution><institution_name>Center for Astrophysics | Harvard &#x26; Smithsonian, 60 Garden St., Cambridge, MA 02138</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0003-3734-3587</ORCID><alt-name><string-name>Peter K. G. Williams</string-name></alt-name></person_name></contributors><titles><title>Making Research Data Flow With Python</title></titles><jats:abstract><jats:p>The increasing volume of research data in fields such as astronomy, biology,
and engineering necessitates efficient distributed data management.
Traditional commercial solutions are often unsuitable for the decentralized
infrastructure typical of academic projects. This paper presents the
Librarian, a custom framework designed for data transfer in large academic
collaborations, designed for the Simons Observatory (SO) as a ground up
re-architechture of a previous astronomical data management tool called the
‘HERA Librarian’ from which it takes its name. SO is a new-generation
observatory designed for observing the Cosmic Microwave Background, and is
located in the Atacama desert in Chile at over 5000 meters of elevation.</jats:p><jats:p>Existing tools like Globus Flows, iRODS, Rucio, and Datalad were evaluated
but were found to be lacking in automation or simplicity. Librarian
addresses these gaps by integrating with Globus for efficient data transfer
and providing a RESTful API for easy interaction. It also supports transfers
through the movement of physical media for environments with intermittent
connectivity.</jats:p><jats:p>Using technologies like Python, FastAPI, and SQLAlchemy, the Librarian
ensures robust, scalable, and user-friendly data management tailored to the
needs of large-scale scientific projects. This solution demonstrates an
effective method for managing the substantial data flows in modern ‘big
science’ endeavors.</jats:p></jats:abstract><publication_date media_type="online"><month>07</month><day>10</day><year>2024</year></publication_date><pages><first_page>236</first_page><last_page>246</last_page></pages><ai:program name="AccessIndicators"><ai:free_to_read></ai:free_to_read><ai:license_ref applies_to="vor">https://creativecommons.org/licenses/by/4.0/</ai:license_ref></ai:program><doi_data><doi>10.25080/HWGA5253</doi><resource content_version="vor">https://doi.curvenote.com/10.25080/HWGA5253</resource></doi_data><citation_list><citation key="Ade2019"><doi>10.1088/1475-7516/2019/02/056</doi></citation><citation key="Foster2011"><doi>10.1109/MIC.2011.64</doi></citation><citation key="Allen2012"><doi>10.1145/2076450.2076468</doi></citation><citation key="Rucio2019"><doi>10.1007/s41781-019-0026-3</doi></citation><citation key="Halchenko2021"><doi>10.21105/joss.03262</doi></citation><citation key="DeBoer2017"><doi>10.1088/1538-3873/129/974/045001</doi></citation><citation key="LaPlante2020"><doi>10.46620/20-0041</doi></citation><citation key="LaPlante2021"><doi>10.1016/j.ascom.2021.100489</doi></citation></citation_list></conference_paper><conference_paper><contributors><person_name sequence="first" contributor_role="author"><given_name>Trevor</given_name><surname>Manz</surname><affiliations><institution><institution_name>Department of Biomedical Informatics, Harvard Medical School, Boston, MA, USA</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0001-7694-5164</ORCID><alt-name><string-name>Trevor Manz</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Nils</given_name><surname>Gehlenborg</surname><affiliations><institution><institution_name>Department of Biomedical Informatics, Harvard Medical School, Boston, MA, USA</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0003-0327-8297</ORCID><alt-name><string-name>Nils Gehlenborg</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Nezar</given_name><surname>Abdennur</surname><affiliations><institution><institution_name>Department of Genomics and Computational Biology, UMass Chan Medical School, Worcester, MA, USA</institution_name></institution><institution><institution_name>Department of Systems Biology, UMass Chan Medical School, Worcester, MA, USA</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0001-5814-0864</ORCID><alt-name><string-name>Nezar Abdennur</string-name></alt-name></person_name></contributors><titles><title>Any notebook served: authoring and sharing reusable interactive widgets</title></titles><jats:abstract><jats:p>The open-source Jupyter project has fostered a robust ecosystem around
notebook-based computing, resulting in diverse Jupyter-compatible platforms
(e.g., JupyterLab, Google Colab, VS Code). Jupyter Widgets extend these
environments with custom visualizations and interactive elements that
communicate directly with user code and data. While this bidirectional
communication makes the widget system powerful, its architecture is currently
tightly coupled to platforms. As a result, widgets are complex and error-prone
to author and distribute, limiting the potential of the wider widget ecosystem.
Here we describe the motivation and approach behind the <jats:italic>anywidget</jats:italic> project,
a specification and toolset for portable and reusable web-based widgets in
interactive computing environments. It ensures cross-platform compatibility
by using the web browser’s built-in module system to load these modules from
the notebook kernel. This design simplifies widget authorship and distribution,
enables rapid prototyping, and lowers the barrier to entry for newcomers.
Anywidget is compatible with not just Jupyter-compatible platforms but any web-based notebook
platform or authoring environment and is already adopted by other projects.
Its adoption has sparked a widget renaissance, improving reusability,
interoperability, and making interactive computing more accessible.</jats:p></jats:abstract><publication_date media_type="online"><month>07</month><day>10</day><year>2024</year></publication_date><pages><first_page>247</first_page><last_page>255</last_page></pages><ai:program name="AccessIndicators"><ai:free_to_read></ai:free_to_read><ai:license_ref applies_to="vor">https://creativecommons.org/licenses/by/4.0/</ai:license_ref></ai:program><doi_data><doi>10.25080/NRPV2311</doi><resource content_version="vor">https://doi.curvenote.com/10.25080/NRPV2311</resource></doi_data><citation_list><citation key="Kluyver2016"><doi>10.3233/978-1-61499-649-1-87</doi></citation><citation key="Granger2021"><doi>10.1109/MCSE.2021.3059263</doi></citation><citation key="manzt2023"><doi>10.1093/bioinformatics/btad050</doi></citation><citation key="viv"><doi>10.1038/s41592-022-01482-7</doi></citation><citation key="vitessce"><doi>10.31219/osf.io/y8thv</doi></citation><citation key="joss"><doi>10.31219/osf.io/tw9sg</doi></citation><citation key="jscatter"><doi>10.21105/joss.07059</doi></citation><citation key="Heer2024"><doi>10.1109/TVCG.2023.3327189</doi></citation><citation key="Gadhave2023"><doi>10.31219/osf.io/9x8eq</doi></citation><citation key="manz2024cev"><doi>10.31219/osf.io/puxnf</doi></citation><citation key="VanderPlas2018"><doi>10.21105/joss.01057</doi></citation><citation key="Wang2024"><doi>10.1145/3613905.3650848</doi></citation></citation_list></conference_paper><conference_paper><contributors><person_name sequence="first" contributor_role="author"><given_name>Matthew</given_name><surname>McCormick</surname><affiliations><institution><institution_name>Kitware, Inc.</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0001-9475-3756</ORCID><alt-name><string-name>Matthew McCormick</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Paul</given_name><surname>Elliott</surname><affiliations><institution><institution_name>Kitware, Inc.</institution_name></institution></affiliations><ORCID>https://orcid.org/0009-0000-8749-3327</ORCID><alt-name><string-name>Paul Elliott</string-name></alt-name></person_name></contributors><titles><title>ITK-Wasm</title><subtitle>Universal spatial analysis and visualization</subtitle></titles><jats:abstract><jats:p>In recent years, WebAssembly (<jats:abbrev alt="WebAssembly">Wasm</jats:abbrev>) has emerged as a widely-supported technology that offers high performance, compact binary size, support for multiple languages, hardware independence, security, and universal platform support, enabling developers to bring near-native speeds and portability to applications for the web and beyond. <jats:abbrev alt="Insight Toolkit">ITK</jats:abbrev>-<jats:abbrev alt="WebAssembly">Wasm</jats:abbrev> brings WebAssembly’s capabilities to scientific computing by combining the Insight Toolkit (<jats:abbrev alt="Insight Toolkit">ITK</jats:abbrev>) and WebAssembly to enable high-performance spatial analysis across programming languages and hardware architectures.</jats:p><jats:p>In the scientific Python ecosystem, <jats:abbrev alt="Insight Toolkit">ITK</jats:abbrev>-<jats:abbrev alt="WebAssembly">Wasm</jats:abbrev> packages work in a web browser via Pyodide but also in system-level environments through the WebAssembly System Interface (<jats:abbrev alt="WebAssembly System Interface">WASI</jats:abbrev>). <jats:abbrev alt="Insight Toolkit">ITK</jats:abbrev>-<jats:abbrev alt="WebAssembly">Wasm</jats:abbrev> bridges WebAssembly with scientific Python through simple, fundamental Python and NumPy-based data structures and Pythonic function interfaces. These interfaces can be accelerated through graphics processing units (<jats:abbrev alt="Graphics Processing Unit">GPU</jats:abbrev>) or neural processing unit (<jats:abbrev alt="Neural Processing Unit">NPU</jats:abbrev>) implementations when available.</jats:p><jats:p>Beyond Python, <jats:abbrev alt="Insight Toolkit">ITK</jats:abbrev>-<jats:abbrev alt="WebAssembly">Wasm</jats:abbrev>’s integration of the WebAssembly Component Model launches scientific computing into a new world of interoperability, enabling the creation of accessible and sustainable multi-language projects that are easily distributed anywhere.</jats:p></jats:abstract><publication_date media_type="online"><month>07</month><day>10</day><year>2024</year></publication_date><pages><first_page>256</first_page><last_page>279</last_page></pages><ai:program name="AccessIndicators"><ai:free_to_read></ai:free_to_read><ai:license_ref applies_to="vor">https://creativecommons.org/licenses/by/4.0/</ai:license_ref></ai:program><doi_data><doi>10.25080/TCFJ5130</doi><resource content_version="vor">https://doi.curvenote.com/10.25080/TCFJ5130</resource></doi_data><citation_list><citation key="10.1145/3140587.3062363"><doi>10.1145/3140587.3062363</doi></citation><citation key="10.1145/2048147.2048224"><doi>10.1145/2048147.2048224</doi></citation><citation key="Mirebeau_2014"><doi>10.54294/en3833</doi></citation><citation key="10.1145/3282510"><doi>10.1145/3282510</doi></citation><citation key="https://doi.org/10.5281/zenodo.4323446"><doi>10.5281/ZENODO.4323446</doi></citation><citation key="10.1145/3572848.3577436"><doi>10.1145/3572848.3577436</doi></citation><citation key="https://doi.org/10.5281/zenodo.3688880"><doi>10.5281/ZENODO.3688880</doi></citation><citation key="McCormick2014-od"><doi>10.3389/fninf.2014.00013</doi></citation><citation key="https://doi.org/10.5281/zenodo.889843"><doi>10.5281/ZENODO.889843</doi></citation><citation key="https://doi.org/10.5281/zenodo.804964"><doi>10.5281/ZENODO.804964</doi></citation><citation key="https://doi.org/10.5281/zenodo.5703670"><doi>10.5281/ZENODO.5703670</doi></citation><citation key="https://doi.org/10.48550/arxiv.2310.18351"><doi>10.48550/ARXIV.2310.18351</doi></citation><citation key="https://doi.org/10.5281/zenodo.10032227"><doi>10.5281/ZENODO.10032227</doi></citation><citation key="Moore_2023"><doi>10.1007/s00418-023-02209-1</doi></citation><citation key="Moore_2021"><doi>10.1038/s41592-021-01326-w</doi></citation><citation key="https://doi.org/10.5281/zenodo.8092821"><doi>10.5281/ZENODO.8092821</doi></citation><citation key="Cardoso_2015"><doi>10.1007/978-3-319-24571-3_81</doi></citation><citation key="Fedorov_2012"><doi>10.1016/j.mri.2012.05.001</doi></citation></citation_list></conference_paper><conference_paper><contributors><person_name sequence="first" contributor_role="author"><given_name>Matthew</given_name><surname>Feickert</surname><affiliations><institution><institution_name>University of Wisconsin--Madison</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0003-4124-7862</ORCID><alt-name><string-name>Matthew Feickert</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Nikolai</given_name><surname>Hartmann</surname><affiliations><institution><institution_name>Ludwig Maximilians Universitat</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0003-0047-2908</ORCID><alt-name><string-name>Nikolai Hartmann</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Lukas</given_name><surname>Heinrich</surname><affiliations><institution><institution_name>Technical University of Munich</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0002-4048-7584</ORCID><alt-name><string-name>Lukas Heinrich</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Alexander</given_name><surname>Held</surname><affiliations><institution><institution_name>University of Wisconsin--Madison</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0002-8924-5885</ORCID><alt-name><string-name>Alexander Held</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Vangelis</given_name><surname>Kourlitis</surname><affiliations><institution><institution_name>Technical University of Munich</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0001-6568-2047</ORCID><alt-name><string-name>Vangelis Kourlitis</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Nils</given_name><surname>Krumnack</surname><affiliations><institution><institution_name>Iowa State University</institution_name></institution></affiliations><alt-name><string-name>Nils Krumnack</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Giordon</given_name><surname>Stark</surname><affiliations><institution><institution_name>Santa Cruz Institute for Particle Physics</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0001-6616-3433</ORCID><alt-name><string-name>Giordon Stark</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Matthias</given_name><surname>Vigl</surname><affiliations><institution><institution_name>Technical University of Munich</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0003-2281-3822</ORCID><alt-name><string-name>Matthias Vigl</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Gordon</given_name><surname>Watts</surname><affiliations><institution><institution_name>University of Washington</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0002-0753-7308</ORCID><alt-name><string-name>Gordon Watts</string-name></alt-name></person_name></contributors><titles><title>How the Scientific Python ecosystem helps answer fundamental questions of the Universe</title></titles><jats:abstract><jats:p>The ATLAS experiment at CERN explores vast amounts of physics data to answer the most fundamental questions of the Universe.
The prevalence of Python in scientific computing motivated ATLAS to adopt it for its data analysis workflows while enhancing users’ experience.
This paper will describe to a broad audience how a large scientific collaboration leverages the power of the Scientific Python ecosystem to tackle domain-specific challenges and advance our understanding of the Cosmos.
Through a simplified example of the renowned Higgs boson discovery, attendees will gain insights into the utilization of Python libraries to discriminate a signal in immersive noise, through tasks such as data cleaning, feature engineering, statistical interpretation and visualization at scale.</jats:p></jats:abstract><publication_date media_type="online"><month>07</month><day>10</day><year>2024</year></publication_date><pages><first_page>280</first_page><last_page>290</last_page></pages><ai:program name="AccessIndicators"><ai:free_to_read></ai:free_to_read><ai:license_ref applies_to="vor">https://creativecommons.org/licenses/by/4.0/</ai:license_ref></ai:program><doi_data><doi>10.25080/KMXN4784</doi><resource content_version="vor">https://doi.curvenote.com/10.25080/KMXN4784</resource></doi_data><citation_list><citation key="PERF-2007-01"><doi>10.1088/1748-0221/3/08/S08003</doi></citation><citation key="Rodrigues:2020syo"><doi>10.1051/epjconf/202024506028</doi></citation><citation key="henry_schreiner-proc-scipy-2022"><doi>10.25080/majora-212e5952-012</doi></citation><citation key="CWPDOC"><doi>10.1007/s41781-018-0018-8</doi></citation><citation key="numpy"><doi>10.1038/s41586-020-2649-2</doi></citation><citation key="Awkward_Array_zenodo"><doi>10.5281/zenodo.4341376</doi></citation><citation key="Hartmann:2021qzp"><doi>10.1051/epjconf/202125103001</doi></citation><citation key="Brun:1997pa"><doi>10.1016/S0168-9002(97)00048-X</doi></citation><citation key="Pivarski:2020qcb"><doi>10.1051/epjconf/202024505023</doi></citation><citation key="Uproot_zenodo"><doi>10.5281/zenodo.4340632</doi></citation><citation key="Athena_zenodo"><doi>10.5281/zenodo.4772550</doi></citation><citation key="Boost-histogram_zenodo"><doi>10.5281/zenodo.3492034</doi></citation><citation key="coffea_zenodo"><doi>10.5281/zenodo.3266454</doi></citation><citation key="hist_zenodo"><doi>10.5281/zenodo.4057112</doi></citation><citation key="mplhep_zenodo"><doi>10.5281/zenodo.6807166</doi></citation><citation key="matplotlib"><doi>10.1109/MCSE.2007.55</doi></citation><citation key="HIGG-2012-27"><doi>10.1016/j.physletb.2012.08.020</doi></citation><citation key="CMS-HIG-12-028"><doi>10.1016/j.physletb.2012.08.021</doi></citation><citation key="IRIS-HEP-AGC"><doi>10.5281/zenodo.7274936</doi></citation><citation key="ATLAS-open-data"><doi>10.7483/OPENDATA.ATLAS.2Y1T.TLGL</doi></citation><citation key="pyhf_zenodo"><doi>10.5281/zenodo.1169739</doi></citation><citation key="pyhf_joss"><doi>10.21105/joss.02823</doi></citation><citation key="cabinetry_zenodo"><doi>10.5281/zenodo.4742752</doi></citation><citation key="iminuit_zenodo"><doi>10.5281/zenodo.3949207</doi></citation></citation_list></conference_paper><conference_paper><contributors><person_name sequence="first" contributor_role="author"><given_name>Nathan</given_name><surname>Martindale</surname><affiliations><institution><institution_name>Oak Ridge National Laboratory</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0002-5036-5433</ORCID><alt-name><string-name>Nathan Martindale</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Jacob</given_name><surname>Smith</surname><affiliations><institution><institution_name>Oak Ridge National Laboratory</institution_name></institution></affiliations><ORCID>https://orcid.org/0009-0000-4565-2692</ORCID><alt-name><string-name>Jacob Smith</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Lisa</given_name><surname>Linville</surname><affiliations><institution><institution_name>Sandia National Laboratories</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0003-1319-1056</ORCID><alt-name><string-name>Lisa Linville</string-name></alt-name></person_name></contributors><titles><title>Supporting Greater Interactivity in the IPython Visualization Ecosystem</title><subtitle>IPyWidgets, IPyVuetify, and beyond</subtitle></titles><jats:abstract><jats:p>Interactive visualizations are invaluable tools for building intuition and
supporting rapid exploration of datasets and models. Numerous libraries in
Python support interactivity, and workflows that combine Jupyter and
IPyWidgets in particular make it straightforward to build data analysis
tools on the fly. However, the field is missing the ability to arbitrarily
overlay widgets and plots on top of others to support more flexible
details-on-demand techniques. This work discusses some limitations of the
base IPyWidgets library, explains the benefits of IPyVuetify and how it
addresses these limitations, and finally presents a new open-source solution
that builds on IPyVuetify to provide easily integrated widget overlays in
Jupyter.</jats:p></jats:abstract><publication_date media_type="online"><month>07</month><day>10</day><year>2024</year></publication_date><pages><first_page>291</first_page><last_page>308</last_page></pages><ai:program name="AccessIndicators"><ai:free_to_read></ai:free_to_read><ai:license_ref applies_to="vor">https://creativecommons.org/licenses/by/4.0/</ai:license_ref></ai:program><doi_data><doi>10.25080/GVHT1072</doi><resource content_version="vor">https://doi.curvenote.com/10.25080/GVHT1072</resource></doi_data><citation_list><citation key="shneidermanEyesHaveIt1996"><doi>10.1109/VL.1996.545307</doi></citation><citation key="ipympl"><doi>10.5281/zenodo.10974323</doi></citation><citation key="altair"><doi>10.21105/joss.01057</doi></citation><citation key="https://doi.org/10.5281/zenodo.1286976"><doi>10.5281/ZENODO.1286976</doi></citation><citation key="https://doi.org/10.5281/zenodo.11182005"><doi>10.5281/ZENODO.11182005</doi></citation><citation key="https://doi.org/10.48550/arxiv.1802.03426"><doi>10.48550/ARXIV.1802.03426</doi></citation></citation_list></conference_paper><conference_paper><contributors><person_name sequence="first" contributor_role="author"><given_name>Pryce</given_name><surname>Turner</surname><affiliations><institution><institution_name>Union AI</institution_name></institution></affiliations><alt-name><string-name>Pryce Turner</string-name></alt-name></person_name></contributors><titles><title>Orchestrating Bioinformatics Workflows Across a Heterogeneous Toolset with Flyte</title></titles><jats:abstract><jats:p>While Python excels at prototyping and iterating quickly, it’s not always performant enough for whole-genome scale data processing. Flyte, an open-source Python-based workflow orchestrator, presents an excellent way to tie together the myriad tools required to run bioinformatics workflows. Flyte is a Kubernetes native orchestrator, meaning all dependencies are captured and versioned in container images. It also allows you to define custom types in Python representing genomic datasets, enabling a powerful way to enforce compatibility across tools. Finally, Flyte provides a number of different abstractions for wrapping these tools, enabling further standardization. Computational biologists, or any scientists processing data with a heterogeneous toolset, stand to benefit from a common orchestration layer that is opinionated yet flexible.</jats:p></jats:abstract><publication_date media_type="online"><month>07</month><day>10</day><year>2024</year></publication_date><pages><first_page>309</first_page><last_page>319</last_page></pages><ai:program name="AccessIndicators"><ai:free_to_read></ai:free_to_read><ai:license_ref applies_to="vor">https://creativecommons.org/licenses/by/4.0/</ai:license_ref></ai:program><doi_data><doi>10.25080/DDJJ4932</doi><resource content_version="vor">https://doi.curvenote.com/10.25080/DDJJ4932</resource></doi_data><citation_list><citation key="Venter_2001"><doi>10.1126/science.1058040</doi></citation><citation key="Van_der_Auwera_2013"><doi>10.1002/0471250953.bi1110s43</doi></citation><citation key="Ewels_2016"><doi>10.1093/bioinformatics/btw354</doi></citation><citation key="Baker_2016"><doi>10.1038/533452a</doi></citation><citation key="Armstrong_2019"><doi>10.1146/annurev-animal-020518-115005</doi></citation><citation key="Venter_1998"><doi>10.1126/science.280.5369.1540</doi></citation><citation key="Langmead_2009"><doi>10.1186/gb-2009-10-3-r25</doi></citation><citation key="Chen_2023"><doi>10.1002/imt2.107</doi></citation></citation_list></conference_paper><conference_paper><contributors><person_name sequence="first" contributor_role="author"><given_name>Sam</given_name><surname>Morley</surname><affiliations><institution><institution_name>University of Oxford</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0001-5971-7418</ORCID><alt-name><string-name>Sam Morley</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Terry</given_name><surname>Lyons</surname><affiliations><institution><institution_name>University of Oxford</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0002-9972-2809</ORCID><alt-name><string-name>Terry Lyons</string-name></alt-name></person_name></contributors><titles><title>RoughPy</title><subtitle>Streaming data is rarely smooth</subtitle></titles><jats:abstract><jats:p>Rough path theory is a branch of mathematics arising out of stochastic
analysis. One of the main tools of rough path analysis is the signature,
which captures the evolution of an unparametrised path including the order in
which events occur. This turns out to be a useful tool in data science
applications involving sequential data. RoughPy is our new Python package
that aims change the way we think about sequential streamed data, by viewing
it through the lens of rough paths. In RoughPy, data is wrapped in a stream
object which can be composed and queried to obtain signatures that can be
used in analysis. It also provides a platform for further exploration of the
connections between rough path theory and data science.</jats:p></jats:abstract><publication_date media_type="online"><month>07</month><day>10</day><year>2024</year></publication_date><pages><first_page>320</first_page><last_page>331</last_page></pages><ai:program name="AccessIndicators"><ai:free_to_read></ai:free_to_read><ai:license_ref applies_to="vor">https://creativecommons.org/licenses/by/4.0/</ai:license_ref></ai:program><doi_data><doi>10.25080/DXWY3560</doi><resource content_version="vor">https://doi.curvenote.com/10.25080/DXWY3560</resource></doi_data><citation_list><citation key="Lyons_2007"><doi>10.1007/978-3-540-71285-5</doi></citation><citation key="Reizenstein_2020"><doi>10.1145/3371237</doi></citation><citation key="https://doi.org/10.48550/arxiv.2001.00706"><doi>10.48550/ARXIV.2001.00706</doi></citation><citation key="https://doi.org/10.48550/arxiv.2206.14674"><doi>10.48550/ARXIV.2206.14674</doi></citation><citation key="Hambly_2010"><doi>10.4007/annals.2010.171.109</doi></citation><citation key="NEURIPS2019_d2cdf047"><doi>10.48550/arXiv.1905.08494</doi></citation><citation key="JMLR_v20_16_314"><doi>10.48550/arXiv.2102.03657</doi></citation><citation key="Cass_2024"><doi>10.1214/23-aap1973</doi></citation><citation key="Salvi_2021"><doi>10.1137/20m1366794</doi></citation><citation key="https://doi.org/10.48550/arxiv.2404.02926"><doi>10.48550/ARXIV.2404.02926</doi></citation><citation key="NEURIPS2020_4a5876b4"><doi>10.48550/arXiv.1906.08215</doi></citation><citation key="https://doi.org/10.48550/arxiv.2402.18512"><doi>10.48550/ARXIV.2402.18512</doi></citation><citation key="https://doi.org/10.48550/arxiv.2402.14892"><doi>10.48550/ARXIV.2402.14892</doi></citation><citation key="https://doi.org/10.48550/arxiv.2006.03487"><doi>10.48550/ARXIV.2006.03487</doi></citation><citation key="Cochrane_2021"><doi>10.1109/csr51186.2021.9527933</doi></citation><citation key="Tseriotou_2023"><doi>10.18653/v1/2023.findings-acl.310</doi></citation><citation key="tseriotou_etal_2024_sig"><doi>10.48550/arXiv.2312.03523</doi></citation><citation key="Ibraheem_2023"><doi>10.1016/j.apenergy.2023.121974</doi></citation><citation key="Morrill_2020"><doi>10.1097/ccm.0000000000004510</doi></citation><citation key="Cohen_2024"><doi>10.1038/s41598-024-51989-6</doi></citation><citation key="Falcioni_2023"><doi>10.1109/medai59581.2023.00008</doi></citation><citation key="Yang_2022"><doi>10.1007/978-3-030-98519-6_18</doi></citation><citation key="Cheng_2024"><doi>10.1109/tmm.2023.3318242</doi></citation><citation key="liao2021a"><doi>10.48550/arXiv.2110.13008</doi></citation><citation key="https://doi.org/10.48550/arxiv.2308.12840"><doi>10.48550/ARXIV.2308.12840</doi></citation><citation key="https://doi.org/10.48550/arxiv.2403.15212"><doi>10.48550/ARXIV.2403.15212</doi></citation><citation key="Ibrahim_2022"><doi>10.1109/cvprw56347.2022.00409</doi></citation><citation key="Xie_2018"><doi>10.1109/tpami.2017.2732978</doi></citation><citation key="Meurer_2023"><doi>10.25080/gerudo-f2bc6f59-001</doi></citation></citation_list></conference_paper><conference_paper><contributors><person_name sequence="first" contributor_role="author"><given_name>Suvrakamal</given_name><surname>Das</surname><affiliations><institution><institution_name>Maulana Abul Kalam Azad University Institute of Technology, West Bengal</institution_name></institution></affiliations><ORCID>https://orcid.org/0009-0002-4791-9244</ORCID><alt-name><string-name>Suvrakamal Das</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Rounak</given_name><surname>Sen</surname><affiliations><institution><institution_name>Maulana Abul Kalam Azad University Institute of Technology, West Bengal</institution_name></institution></affiliations><ORCID>https://orcid.org/0009-0003-9327-4712</ORCID><alt-name><string-name>Rounak Sen</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Saikrishna</given_name><surname>Devendiran</surname><affiliations><institution><institution_name>Amrita School of Computing, Amritapuri</institution_name></institution></affiliations><ORCID>https://orcid.org/0009-0003-6153-3177</ORCID><alt-name><string-name>Saikrishna Devendiran</string-name></alt-name></person_name></contributors><titles><title>Mamba Models a possible replacement for Transformers?</title><subtitle>A Memory-Efficient Approach for Scientific Computing</subtitle></titles><jats:abstract><jats:p>The quest for more efficient and faster deep learning models has led to the development of various alternatives to Transformers, one of which is the Mamba model. This paper provides a comprehensive comparison between Mamba models and Transformers, focusing on their architectural differences, performance metrics, and underlying mechanisms. It analyzes and synthesizes findings from extensive research conducted by various authors on these models. The synergy between Mamba models and the SciPy ecosystem enhances their integration into science. By providing an in-depth comparison using Python and its scientific ecosystem, this paper aims to clarify the strengths and weaknesses of Mamba models relative to Transformers. It offers the results obtained along with some thoughts on the possible ramifications for future research and applications in a range of academic and professional fields.</jats:p></jats:abstract><publication_date media_type="online"><month>07</month><day>10</day><year>2024</year></publication_date><pages><first_page>332</first_page><last_page>344</last_page></pages><ai:program name="AccessIndicators"><ai:free_to_read></ai:free_to_read><ai:license_ref applies_to="vor">https://creativecommons.org/licenses/by/4.0/</ai:license_ref></ai:program><doi_data><doi>10.25080/XHDR4700</doi><resource content_version="vor">https://doi.curvenote.com/10.25080/XHDR4700</resource></doi_data><citation_list><citation key="Sherstinsky_2020"><doi>10.1016/j.physd.2019.132306</doi></citation><citation key="oshea2015introduction"><doi>10.48550/arXiv.1511.08458</doi></citation><citation key="vaswani2023attention"><doi>10.48550/arXiv.1706.03762</doi></citation><citation key="gu2022efficiently"><doi>10.48550/arXiv.2111.00396</doi></citation><citation key="gu2020hippo"><doi>10.48550/arXiv.2008.07669</doi></citation><citation key="gu2022parameterization"><doi>10.48550/arXiv.2206.11893</doi></citation><citation key="lim2024parallelizing"><doi>10.48550/arXiv.2309.12252</doi></citation><citation key="gu2023mamba"><doi>10.48550/arXiv.2312.00752</doi></citation><citation key="elfwing2017sigmoidweighted"><doi>10.48550/arXiv.1702.03118</doi></citation><citation key="sun2023retentive"><doi>10.48550/arXiv.2307.08621</doi></citation><citation key="de2024griffin"><doi>10.48550/arXiv.2402.19427</doi></citation><citation key="poli2023hyena"><doi>10.48550/arXiv.2302.10866</doi></citation><citation key="peng2023rwkv"><doi>10.48550/arXiv.2305.13048</doi></citation><citation key="scipy"><doi>10.1038/s41592-019-0686-2</doi></citation><citation key="ma2024umamba"><doi>10.48550/arXiv.2401.04722</doi></citation><citation key="zhu2024vision"><doi>10.48550/arXiv.2401.09417</doi></citation><citation key="liu2024vmamba"><doi>10.48550/arXiv.2401.10166</doi></citation><citation key="wang2024mambabyte"><doi>10.48550/arXiv.2401.13660</doi></citation><citation key="lieber2024jamba"><doi>10.48550/arXiv.2403.19887</doi></citation></citation_list></conference_paper><conference_paper><contributors><person_name sequence="first" contributor_role="author"><given_name>Samuel</given_name><surname>Williams</surname><affiliations><institution><institution_name>U.S. Army Engineer Research &#x26; Development Center</institution_name></institution><institution><institution_name>U.S. Army Corps of Engineers</institution_name></institution></affiliations><alt-name><string-name>Samuel Williams</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Scott</given_name><surname>Christensen</surname><affiliations><institution><institution_name>U.S. Army Engineer Research &#x26; Development Center</institution_name></institution><institution><institution_name>U.S. Army Corps of Engineers</institution_name></institution></affiliations><alt-name><string-name>Scott Christensen</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Marvin</given_name><surname>Brown</surname><affiliations><institution><institution_name>U.S. Army Engineer Research &#x26; Development Center</institution_name></institution><institution><institution_name>U.S. Army Corps of Engineers</institution_name></institution></affiliations><alt-name><string-name>Marvin Brown</string-name></alt-name></person_name></contributors><titles><title>THEIA: An Offline Tool for Tradespace Visualization</title></titles><jats:abstract><jats:p>Within the Army Corps of Engineers (<jats:abbrev alt="United States Army Corps of Engineers">USACE</jats:abbrev>), there is a need to evaluate tradespaces. Tradespace datasets are the result of large parameter sweeps run over numerous design options and can consist of thousands or even millions of design configurations and the corresponding performance metrics. Because of the immense size of these datasets, the ability to effectively visualize the data is essential for proper evaluation. At the <jats:abbrev alt="United States Army Corps of Engineers">USACE</jats:abbrev> Engineer Research &#x26;amp; Development Center (<jats:abbrev alt="U.S. Army Engineer Research &#x26; Development Center">ERDC</jats:abbrev>), an easy-to-use plotting tool known as the Tradespace Holistic Exploration &#x26;amp; Insight Application (<jats:abbrev alt="Tradespace Holistic Exploration &#x26; Insight Application">THEIA</jats:abbrev>) has been developed for visualizing this complex tradespace data related to the acquisitions process. <jats:abbrev alt="Tradespace Holistic Exploration &#x26; Insight Application">THEIA</jats:abbrev> was developed using Python libraries including Panel, Param, Holoviews, Bokeh, and Plotly. When combined, these libraries offer a wide range of widgets and plots that allow the user to visualize their data in multiple ways. Additionally, users can easily save plots, export findings, and utilize multiple data files at once. <jats:abbrev alt="Tradespace Holistic Exploration &#x26; Insight Application">THEIA</jats:abbrev> is also capable of importing tabular data while presenting options to customize visualizations and help the end-user make informed decisions.</jats:p></jats:abstract><publication_date media_type="online"><month>07</month><day>10</day><year>2024</year></publication_date><pages><first_page>345</first_page><last_page>348</last_page></pages><ai:program name="AccessIndicators"><ai:free_to_read></ai:free_to_read><ai:license_ref applies_to="vor">https://creativecommons.org/licenses/by/4.0/</ai:license_ref></ai:program><doi_data><doi>10.25080/RVRR7774</doi><resource content_version="vor">https://doi.curvenote.com/10.25080/RVRR7774</resource></doi_data><citation_list><citation key="panel"><doi>10.5281/zenodo.3706648</doi></citation><citation key="param"><doi>10.5281/zenodo.11046027</doi></citation><citation key="holoviews"><doi>10.5281/zenodo.10653612</doi></citation></citation_list></conference_paper><conference_paper><contributors><person_name sequence="first" contributor_role="author"><given_name>Valentina</given_name><surname>Staneva</surname><affiliations><institution><institution_name>eScience Institute, University of Washington</institution_name></institution></affiliations><alt-name><string-name>Valentina Staneva</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Soham</given_name><surname>Butala</surname><affiliations><institution><institution_name>eScience Institute, University of Washington</institution_name></institution></affiliations><alt-name><string-name>Soham Butala</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Landung (Don)</given_name><surname>Setiawan</surname><affiliations><institution><institution_name>Scientific Software Engineering Center, University of Washington</institution_name></institution></affiliations><alt-name><string-name>Landung (Don) Setiawan</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Wu-Jung</given_name><surname>Lee</surname><affiliations><institution><institution_name>Applied Physics Laboratory, University of Washington</institution_name></institution></affiliations><alt-name><string-name>Wu-Jung Lee</string-name></alt-name></person_name></contributors><titles><title>Echodataflow: Recipe-based Fisheries Acoustics Workflow Orchestration</title></titles><jats:abstract><jats:p>With the influx of large data from multiple instruments and experiments, scientists are wrangling complex data pipelines that are context-dependent and non-reproducible. We demonstrate how we leverage Prefect (Prefect, 2024), a modern orchestration framework, to facilitate fisheries acoustics data processing. We built a Python package Echodataflow (Echodataflow, 2024) which 1) allows users to specify workflows and their parameters through editing text “recipes” which provide transparency and reproducibility of the pipelines; 2) supports scaling of the workflows while abstracting the computational infrastructure; 3) provides monitoring and logging of the workflow progress. Under the hood, Echodataflow uses Prefect to execute the workflows while providing a domain-friendly interface to facilitate diverse fisheries acoustics use cases. We demonstrate the features through a typical ship survey data processing pipeline.</jats:p></jats:abstract><publication_date media_type="online"><month>07</month><day>10</day><year>2024</year></publication_date><pages><first_page>349</first_page><last_page>365</last_page></pages><ai:program name="AccessIndicators"><ai:free_to_read></ai:free_to_read><ai:license_ref applies_to="vor">https://creativecommons.org/licenses/by/4.0/</ai:license_ref></ai:program><doi_data><doi>10.25080/JXDK4427</doi><resource content_version="vor">https://doi.curvenote.com/10.25080/JXDK4427</resource></doi_data><citation_list><citation key="NWFSC_FRAM_2022"><doi>10.25923/0979-6D84</doi></citation><citation key="trowbridge_ooi_2019"><doi>10.3389/fmars.2019.00074</doi></citation><citation key="ladroit_esp3_2020"><doi>10.1016/j.softx.2020.100581</doi></citation><citation key="perrot_matecho_2018"><doi>10.1007/s40857-018-0135-x</doi></citation><citation key="harrison_echoviewR_2015"><doi>10.3389/fmars.2015.00015</doi></citation><citation key="wall_pyecholab_2018"><doi>10.1121/1.5067860</doi></citation><citation key="lee_echopype_2021"><doi>10.48550/arXiv.2111.00187</doi></citation><citation key="numpy"><doi>10.1038/s41586-020-2649-2</doi></citation><citation key="zarr"><doi>10.5281/zenodo.11320255</doi></citation><citation key="xarray"><doi>10.5334/jors.148</doi></citation><citation key="wall_2016"><doi>10.1029/2016eo057595</doi></citation><citation key="pangeo-forge"><doi>10.3389/fclim.2021.782909</doi></citation><citation key="conda_forge_community_2015_4774216"><doi>10.5281/zenodo.4774216</doi></citation><citation key="netcdf"><doi>10.1109/38.56302</doi></citation><citation key="convention"><doi>10.17895/ices.pub.4392</doi></citation><citation key="echoregions"><doi>10.5281/zenodo.8400850</doi></citation><citation key="echopop"><doi>10.5281/zenodo.11454149</doi></citation><citation key="echoshader"><doi>10.5281/zenodo.10856784</doi></citation><citation key="10.1145/3437359.3465565"><doi>10.1145/3437359.3465565</doi></citation><citation key="10.1145/3569951.3597559"><doi>10.1145/3569951.3597559</doi></citation></citation_list></conference_paper><conference_paper><contributors><person_name sequence="first" contributor_role="author"><given_name>Valeria</given_name><surname>Martin</surname><affiliations><institution><institution_name>University of West Florida</institution_name></institution></affiliations><ORCID>https://orcid.org/0009-0000-3668-5003</ORCID><alt-name><string-name>Valeria Martin</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Derek</given_name><surname>Morgan</surname><affiliations><institution><institution_name>University of West Florida</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0003-2321-3765</ORCID><alt-name><string-name>Derek Morgan</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>K. Brent</given_name><surname>Venable</surname><affiliations><institution><institution_name>University of West Florida</institution_name></institution><institution><institution_name>Florida Institute of Human Machine Cognition</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0002-1092-9759</ORCID><alt-name><string-name>K. Brent Venable</string-name></alt-name></person_name></contributors><titles><title>Python-Based GeoImagery Dataset Development for Deep Learning-Driven Forest Wildfire Detection</title></titles><jats:abstract><jats:p>In recent years, leveraging satellite imagery with deep learning (<jats:abbrev alt="Deep Learning">DL</jats:abbrev>) architectures has become an effective approach for environmental monitoring tasks, including forest wildfire detection. Nevertheless, this integration requires substantial high-quality labeled data to train the <jats:abbrev alt="Deep Learning">DL</jats:abbrev> models accurately. Leveraging the capabilities of multiple Python libraries, such as rasterio and GeoPandas, and Google Earth Engine’s Python <jats:abbrev alt="Application Programming Interface">API</jats:abbrev>, this study introduces a streamlined methodology to efficiently gather, label, augment, process, and evaluate a large-scale bi-temporal high-resolution satellite imagery dataset for <jats:abbrev alt="Deep Learning">DL</jats:abbrev>-driven forest wildfire detection. Known as the California Wildfire GeoImaging Dataset (<jats:abbrev alt="California Wildfire GeoImaging Dataset">CWGID</jats:abbrev>), this dataset comprises over 100,000 labeled ’before’ and ’after’ wildfire image pairs, created from pre-existing satellite imagery. An analysis of the dataset using pre-trained and adapted Convolutional Neural Network (<jats:abbrev alt="Convolutional Neural Network">CNN</jats:abbrev>) architectures, such as VGG16 and EfficientNet, achieved accuracies of respectively 76% and 93%. The pipeline outlined in this paper demonstrates how Python can be used to gather and process high-resolution satellite imagery datasets, leading to accurate wildfire detection and providing a tool for broader environmental monitoring.</jats:p></jats:abstract><publication_date media_type="online"><month>07</month><day>10</day><year>2024</year></publication_date><pages><first_page>366</first_page><last_page>385</last_page></pages><ai:program name="AccessIndicators"><ai:free_to_read></ai:free_to_read><ai:license_ref applies_to="vor">https://creativecommons.org/licenses/by/4.0/</ai:license_ref></ai:program><doi_data><doi>10.25080/YADT7194</doi><resource content_version="vor">https://doi.curvenote.com/10.25080/YADT7194</resource></doi_data><citation_list><citation key="8113128"><doi>10.1109/MGRS.2017.2762307</doi></citation><citation key="Massey2023"><doi>10.1007/978-3-031-15988-6_26</doi></citation><citation key="rs14071552"><doi>10.3390/rs14071552</doi></citation><citation key="DBLP:RonnebergerFB15"><doi>10.1007/978-3-319-24574-4_28</doi></citation><citation key="eleo"><doi>10.3390/rs15082092</doi></citation><citation key="al-dabbagh2023uni"><doi>10.1080/19475705.2023.2196370</doi></citation><citation key="Alzubaidi2021ReviewOD"><doi>10.1186/s40537-021-00444-8</doi></citation><citation key="Adegun2023"><doi>10.1186/s40537-023-00772-x</doi></citation><citation key="gorelick2017google"><doi>10.1016/j.rse.2017.06.031</doi></citation><citation key="tensorflow2015-whitepaper"><doi>10.5281/zenodo.4724125</doi></citation><citation key="pandas1"><doi>10.5281/zenodo.3509134</doi></citation><citation key="DRUSCH201225"><doi>10.1016/j.rse.2011.11.026</doi></citation><citation key="pyproj2023"><doi>10.5281/zenodo.8365173</doi></citation><citation key="geopy"><doi>10.21125/inted.2020.0687</doi></citation><citation key="geopandas"><doi>10.5281/zenodo.3946761</doi></citation><citation key="shapely"><doi>10.5281/zenodo.7428463</doi></citation><citation key="hu2015transferring"><doi>10.3390/rs71114680</doi></citation><citation key="marmanis2016deep"><doi>10.1109/LGRS.2015.2499239</doi></citation><citation key="tifffile"><doi>10.5281/zenodo.6795861</doi></citation><citation key="numpy"><doi>10.1038/s41586-020-2649-2</doi></citation><citation key="SEYDI2022108999"><doi>10.1016/j.ecolind.2022.108999</doi></citation><citation key="Hunan"><doi>10.3390/rs15030628</doi></citation></citation_list></conference_paper><conference_paper><contributors><person_name sequence="first" contributor_role="author"><given_name>Wu-Jung</given_name><surname>Lee</surname><affiliations><institution><institution_name>Applied Physics Laboratory, University of Washington</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0002-4112-2034</ORCID><alt-name><string-name>Wu-Jung Lee</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Valentina</given_name><surname>Staneva</surname><affiliations><institution><institution_name>eScience Institute, University of Washington</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0002-3412-0364</ORCID><alt-name><string-name>Valentina Staneva</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Landung “Don”</given_name><surname>Setiawan</surname><affiliations><institution><institution_name>eScience Institute, University of Washington</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0002-1624-2667</ORCID><alt-name><string-name>Landung “Don” Setiawan</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Emilio</given_name><surname>Mayorga</surname><affiliations><institution><institution_name>Applied Physics Laboratory, University of Washington</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0003-2574-4623</ORCID><alt-name><string-name>Emilio Mayorga</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Caesar</given_name><surname>Tuguinay</surname><affiliations><institution><institution_name>Applied Physics Laboratory, University of Washington</institution_name></institution></affiliations><ORCID>https://orcid.org/0009-0000-6652-1093</ORCID><alt-name><string-name>Caesar Tuguinay</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Soham</given_name><surname>Butala</surname><affiliations><institution><institution_name>eScience Institute, University of Washington</institution_name></institution></affiliations><ORCID>https://orcid.org/0009-0000-6479-889X</ORCID><alt-name><string-name>Soham Butala</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Brandyn</given_name><surname>Lucca</surname><affiliations><institution><institution_name>Applied Physics Laboratory, University of Washington</institution_name></institution></affiliations><ORCID>https://orcid.org/0000-0003-3145-2969</ORCID><alt-name><string-name>Brandyn Lucca</string-name></alt-name></person_name><person_name sequence="additional" contributor_role="author"><given_name>Dingrui</given_name><surname>Lei</surname><affiliations><institution><institution_name>eScience Institute, University of Washington</institution_name></institution></affiliations><ORCID>https://orcid.org/0009-0003-8543-3138</ORCID><alt-name><string-name>Dingrui Lei</string-name></alt-name></person_name></contributors><titles><title>Echostack: A flexible and scalable open-source software suite for echosounder data processing</title></titles><jats:abstract><jats:p>Water column sonar data collected by echosounders are essential for fisheries and marine ecosystem research, enabling the detection, classification, and quantification of fish and zooplankton from many different ocean observing platforms. However, the broad usage of these data has been hindered by the lack of modular software tools that allow flexible composition of data processing workflows that incorporate powerful analytical tools in the scientific Python ecosystem. We address this gap by developing Echostack, a suite of open-source Python software packages that leverage existing distributed computing and cloud-interfacing libraries to support intuitive and scalable data access, processing, and interpretation. These tools can be used individually or orchestrated together, which we demonstrate in example use cases for a fisheries acoustic-trawl survey.</jats:p></jats:abstract><publication_date media_type="online"><month>07</month><day>10</day><year>2024</year></publication_date><pages><first_page>386</first_page><last_page>397</last_page></pages><ai:program name="AccessIndicators"><ai:free_to_read></ai:free_to_read><ai:license_ref applies_to="vor">https://creativecommons.org/licenses/by/4.0/</ai:license_ref></ai:program><doi_data><doi>10.25080/WXRH8633</doi><resource content_version="vor">https://doi.curvenote.com/10.25080/WXRH8633</resource></doi_data><citation_list><citation key="medwin1998"><doi>10.1016/B978-0-12-487570-8.X5000-4</doi></citation><citation key="stanton2012"><doi>10.1016/j.mio.2012.07.002</doi></citation><citation key="benoitbird2016"><doi>10.1146/annurev-marine-122414-034001</doi></citation><citation key="wall2016"><doi>10.1093/icesjms/fsw014</doi></citation><citation key="klevjer2016"><doi>10.1038/srep19873</doi></citation><citation key="perrot_matecho_2018"><doi>10.1007/s40857-018-0135-x</doi></citation><citation key="ladroit_esp3_2020"><doi>10.1016/j.softx.2020.100581</doi></citation><citation key="bednar2023"><doi>10.25080/gerudo-f2bc6f59-00b</doi></citation><citation key="haris2021"><doi>10.1038/s41597-020-00785-8</doi></citation><citation key="trowbridge_ooi_2019"><doi>10.3389/fmars.2019.00074</doi></citation><citation key="moline2015"><doi>10.1175/JTECH-D-15-0035.1</doi></citation><citation key="brautaset2020"><doi>10.1093/ICESJMS/FSZ235</doi></citation><citation key="choi2023"><doi>10.1109/JOE.2022.3226214</doi></citation><citation key="urmy2023"><doi>10.1093/icesjms/fsad102</doi></citation><citation key="zhang2024"><doi>10.1002/lom3.10611</doi></citation><citation key="harrison_echoviewR_2015"><doi>10.3389/fmars.2015.00015</doi></citation><citation key="lee_echopype_2021"><doi>10.48550/arXiv.2111.00187</doi></citation><citation key="hauser_regionmask_2024"><doi>10.5281/zenodo.10849860</doi></citation><citation key="williams2016"><doi>10.1016/j.mio.2016.09.008</doi></citation><citation key="weaver2014"><doi>10.1007/978-0-387-36699-9_36</doi></citation><citation key="NWFSC_FRAM_2022"><doi>10.25923/0979-6D84</doi></citation></citation_list></conference_paper></conference></body></doi_batch>