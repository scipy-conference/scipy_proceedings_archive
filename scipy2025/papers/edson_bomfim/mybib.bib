@article{HEINRICH201895,
    title    = {Assessing data quality – A probability-based metric for semantic consistency},
    journal  = {Decision Support Systems},
    volume   = {110},
    pages    = {95-106},
    year     = {2018},
    issn     = {0167-9236},
    doi      = {https://doi.org/10.1016/j.dss.2018.03.011},
    url      = {https://www.sciencedirect.com/science/article/pii/S0167923618300599},
    author   = {Bernd Heinrich and Mathias Klier and Alexander Schiller and Gerit Wagner},
    keywords = {Data quality, Data quality assessment, Data quality metric, Data consistency},
    abstract = {We present a probability-based metric for semantic consistency using a set of uncertain rules. As opposed to existing metrics for semantic consistency, our metric allows to consider rules that are expected to be fulfilled with specific probabilities. The resulting metric values represent the probability that the assessed dataset is free of internal contradictions with regard to the uncertain rules and thus have a clear interpretation. The theoretical basis for determining the metric values are statistical tests and the concept of the p-value, allowing the interpretation of the metric value as a probability. We demonstrate the practical applicability and effectiveness of the metric in a real-world setting by analyzing a customer dataset of an insurance company. Here, the metric was applied to identify semantic consistency problems in the data and to support decision-making, for instance, when offering individual products to customers.}
}

@misc{chug2021statisticallearningoperationalizedomain,
    title         = {Statistical Learning to Operationalize a Domain Agnostic Data Quality Scoring},
    author        = {Sezal Chug and Priya Kaushal and Ponnurangam Kumaraguru and Tavpritesh Sethi},
    year          = {2021},
    eprint        = {2108.08905},
    archivePrefix = {arXiv},
    primaryClass  = {cs.LG},
    url           = {https://arxiv.org/abs/2108.08905},
}

@inproceedings{9307671,
    author={Picard, S. and Chapdelaine, C. and Cappi, C. and Gardes, L. and Jenn, E. and Lefevre, B. and Soumarmon, T.},
    booktitle={2020 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)},
    title={Ensuring Dataset Quality for Machine Learning Certification},
    year={2020},
    volume={},
    number={},
    pages={275-282},
    keywords={Standards;Data integrity;Data processing;Machine learning;Safety;Databases;Rail transportation;datasets;certification process;machine learning},
    doi={10.1109/ISSREW51248.2020.00085}
}

@article{10.1145/3190578,
    author = {Bors, Christian and Gschwandtner, Theresia and Kriglstein, Simone and Miksch, Silvia and Pohl, Margit},
    title = {Visual Interactive Creation, Customization, and Analysis of Data Quality Metrics},
    year = {2018},
    issue_date = {March 2018},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {10},
    number = {1},
    issn = {1936-1955},
    url = {https://doi.org/10.1145/3190578},
    doi = {10.1145/3190578},
    abstract = {During data preprocessing, analysts spend a significant part of their time and effort profiling the quality of the data along with cleansing and transforming the data for further analysis. While quality metrics—ranging from general to domain-specific measures—support assessment of the quality of a dataset, there are hardly any approaches to visually support the analyst in customizing and applying such metrics. Yet, visual approaches could facilitate users’ involvement in data quality assessment. We present MetricDoc, an interactive environment for assessing data quality that provides customizable, reusable quality metrics in combination with immediate visual feedback. Moreover, we provide an overview visualization of these quality metrics along with error visualizations that facilitate interactive navigation of the data to determine the causes of quality issues present in the data. In this article, we describe the architecture, design, and evaluation of MetricDoc, which underwent several design cycles, including heuristic evaluation and expert reviews as well as a focus group with data quality, human-computer interaction, and visual analytics experts.},
    journal = {J. Data and Information Quality},
    month = may,
    articleno = {3},
    numpages = {26},
    keywords = {Data profiling, data quality metrics, visual exploration}
}

@article{problems-methods-data-cleasing,
    author = {Müller, Heiko and Freytag, Johann-Christoph},
    year = {2003},
    month = {01},
    pages = {},
    title = {Problems, methods, and challenges in comprehensive data cleansing}
}

@article{taxonomy-of-data-quality-problems,
    author = {Oliveira, Paulo and Rodrigues, Fátima and Rangel Henriques, Pedro and Galhardas, Helena},
    year = {2005},
    month = {01},
    pages = {},
    title = {A Taxonomy of Data Quality Problems},
    journal = {Journal of Data and Information Quality - JDIQ}
}

@article{data-cleaning-current-approaches,
    author = {Rahm, Erhard and Do, Hong},
    year = {2000},
    month = {01},
    pages = {3-13},
    title = {Data Cleaning: Problems and Current Approaches},
    volume = {23},
    journal = {IEEE Data Eng. Bull.}
}

@article{taxonomy-of-dity-data,
    author = {Kim, Won and Choi, Byoung-Ju and Hong, Eui and Kim, Soo-Kyung and Lee, Doheon},
    year = {2003},
    month = {01},
    pages = {81-99},
    title = {A Taxonomy of Dirty Data},
    volume = {7},
    journal = {Data Min. Knowl. Discov.},
    doi = {10.1023/A:1021564703268}
}

@inproceedings{taxonomy-data-quality-challenges,
    author = {Bosu, Michael and MacDonell, Stephen},
    year = {2013},
    month = {06},
    pages = {97-106},
    title = {A Taxonomy of Data Quality Challenges in Empirical Software Engineering},
    journal = {Proceedings of the Australian Software Engineering Conference, ASWEC},
    doi = {10.1109/ASWEC.2013.21}
}

@article{survey-of-data-quality-tools,
    author = {Barateiro, José and Galhardas, Helena},
    year = {2005},
    month = {01},
    pages = {15-21},
    title = {A Survey of Data Quality Tools.},
    volume = {14},
    journal = {Datenbank-Spektrum}
}

@inproceedings{what-is-in-our-datasets,
    author = {Rosli, Marshima Mohd and Tempero, Ewan and Luxton-Reilly, Andrew},
    title = {What is in our datasets? describing a structure of datasets},
    year = {2016},
    isbn = {9781450340427},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2843043.2843059},
    doi = {10.1145/2843043.2843059},
    abstract = {In order to facilitate research based on datasets in empirical software engineering, the meaning of data must be able to be interpreted correctly. Datasets contain measurements that are associated with metrics and entities. In some datasets, it is not always clear which entities have been measured and exactly which metrics have been used. This means that measurements could be misinterpreted. The goal of this study is to determine a useful way to understand what datasets are actually intended to represent. We construct precise definitions of datasets and their potential elements. We develop a metamodel to describe the structure and concepts in a dataset, and the relationships between each concept. We apply the metamodel to a number of existing datasets from the PROMISE repository. We found that of the 70 existing datasets we studied, 61 datasets contained insufficient information to ensure correct interpretation for metrics and entities. Our metamodel can be used to identify such datasets and can be used to evaluate new datasets. It will also form the foundation for a framework to evaluate the quality of datasets.},
    booktitle = {Proceedings of the Australasian Computer Science Week Multiconference},
    articleno = {28},
    numpages = {10},
    keywords = {data quality, empirical studies, modeling datasets, software engineering datasets},
    location = {Canberra, Australia},
    series = {ACSW '16}
}

@article{evaluating-the-quality-of-datasets,
    author = {Mohd Rosli, Marshima and Tempero, Ewan and Luxton-Reilly, Andrew},
    year = {2018},
    month = {10},
    pages = {7232-7239},
    title = {Evaluating the Quality of Datasets in Software Engineering},
    volume = {24},
    journal = {Advanced Science Letters},
    doi = {10.1166/asl.2018.12920}
}

@misc{bhatia2024dataqualityantipatternssoftware,
    title         = {Data Quality Antipatterns for Software Analytics}, 
    author        = {Aaditya Bhatia and Dayi Lin and Gopi Krishnan Rajbahadur and Bram Adams and Ahmed E. Hassan},
    year          = {2024},
    eprint        = {2408.12560},
    archivePrefix = {arXiv},
    primaryClass  = {cs.SE},
    url           = {https://arxiv.org/abs/2408.12560}, 
    doi           = {10.48550/arXiv.2408.12560}
}

@article{RIDZUAN2024341,
    title = {A Review on Data Quality Dimensions for Big Data},
    journal = {Procedia Computer Science},
    volume = {234},
    pages = {341-348},
    year = {2024},
    note = {Seventh Information Systems International Conference (ISICO 2023)},
    issn = {1877-0509},
    doi = {https://doi.org/10.1016/j.procs.2024.03.008},
    url = {https://www.sciencedirect.com/science/article/pii/S187705092400365X},
    author = {Fakhitah Ridzuan and Wan Mohd Nazmee Wan Zainon},
    keywords = {big data, data quality, data quality dimension, data management},
    abstract = {Big Data wave has led to a rapid increase in the amount of data being collected by organizations. While the accuracy and reliability of prediction models are often prioritized, the quality of the collected data is frequently overlooked. Poor data quality can result in the common problem of ‘garbage in, garbage out’. Traditional measures of data quality, such as accuracy, consistency, completeness, and timeliness, are no longer adequate in the era of Big Data. Therefore, this paper proposes a taxonomy of data quality dimensions specifically for Big Data, addressing emerging challenges by formulating 20 dimensions and categorizing them into four distinct categories.}
}

@article{Cai-2015,
    abstract = {High-quality data are the precondition for analyzing and using big data and for guaranteeing the value of the data. Currently, comprehensive analysis and research of quality standards and quality assessment methods for big data are lacking. First, this paper summarizes reviews of data quality research. Second, this paper analyzes the data characteristics of the big data environment, presents quality challenges faced by big data, and formulates a hierarchical data quality framework from the perspective of data users. This framework consists of big data quality dimensions, quality characteristics, and quality indexes. Finally, on the basis of this framework, this paper constructs a dynamic assessment process for data quality. This process has good expansibility and adaptability and can meet the needs of big data quality assessment. The research results enrich the theoretical scope of big data and lay a solid foundation for the future by establishing an assessment model and studying evaluation algorithms.},
    author = {Cai, Li and Zhu, Yangyong},
    doi = {10.5334/dsj-2015-002},
    journal = {Data Science Journal},
    month = {May},
    title = {The Challenges of Data Quality and Data Quality Assessment in the Big Data Era},
    year = {2015}
}

@article{10.1145/335191.336568,
author = {Galhardas, Helena and Florescu, Daniela and Shasha, Dennis and Simon, Eric},
title = {AJAX: an extensible data cleaning tool},
year = {2000},
issue_date = {June 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/335191.336568},
doi = {10.1145/335191.336568},
abstract = {@@@@ groups together matching pairs with a high similarity value by applying a given grouping criteria (e.g. by transitive closure). Finally, ging collapses each individual cluster into a tuple of the resulting data source. AJAX provides @@@@ for specifying data cleaning programs, which consists of SQL statements enriched with a set of specific primitives to express these transformations.AJAX also @@@@. It allows the user to interact with an executing data cleaning program to handle exceptional cases and to inspect intermediate results. Finally, AJAX provides @@@@ @@@@ that permits users to determine the source and processing of data for debugging purposes.We will present the AJAX system applied to two real world problems: the consolidation of a telecommunication database, and the conversion of a dirty database of bibliographic references into a set of clean, normalized, and redundancy free relational tables maintaining the same data.},
journal = {SIGMOD Rec.},
month = may,
pages = {590},
numpages = {1}
}

@inproceedings{ajax,
    author = {Galhardas, Helena and Florescu, Daniela and Shasha, Dennis and Simon, Eric},
    title = {AJAX: an extensible data cleaning tool},
    year = {2000},
    isbn = {1581132174},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/342009.336568},
    doi = {10.1145/342009.336568},
    abstract = {@@@@ groups together matching pairs with a high similarity value by applying a given grouping criteria (e.g. by transitive closure). Finally, ging collapses each individual cluster into a tuple of the resulting data source. AJAX provides @@@@ for specifying data cleaning programs, which consists of SQL statements enriched with a set of specific primitives to express these transformations.AJAX also @@@@. It allows the user to interact with an executing data cleaning program to handle exceptional cases and to inspect intermediate results. Finally, AJAX provides @@@@ @@@@ that permits users to determine the source and processing of data for debugging purposes.We will present the AJAX system applied to two real world problems: the consolidation of a telecommunication database, and the conversion of a dirty database of bibliographic references into a set of clean, normalized, and redundancy free relational tables maintaining the same data.},
    booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
    pages = {590},
    location = {Dallas, Texas, USA},
    series = {SIGMOD '00}
}

@inproceedings{the-field-matching-problem,
    author = {Monge, Alvaro E. and Elkan, Charles P.},
    title = {The field matching problem: Algorithms and applications},
    year = {1996},
    publisher = {AAAI Press},
    abstract = {To combine information from heterogeneous sources, equivalent data in the multiple sources must be identified. This task is the field matching problem. Specifically, the task is to determine whether or not two syntactic values are alternative designations of the same semantic entity. For example the addresses Dept. of Comput. Sci. and Eng., University of California, San Diego, 9500 Gilman Dr. Dept. 0114, La Jolla. CA 92093 and UCSD, Computer Science and Engineering Department, CA 92093-0114 do designate the same department. This paper describes three field matching algorithms, and evaluates their performance on real-world datasets. One proposed method is the well-known Smith-Waterman algorithm for comparing DNA and protein sequences. Several applications of field matching in knowledge discovery are described briefly, including WEBFIND, which is a new software tool that discovers scientific papers published on the worldwide web. WEBFIND uses external information sources to guide its search for authors and papers. Like many other worldwide web tools, WEBFIND needs to solve the field matching problem in order to navigate between information sources.},
    booktitle = {Proceedings of the Second International Conference on Knowledge Discovery and Data Mining},
    pages = {267–270},
    numpages = {4},
    location = {Portland, Oregon},
    series = {KDD'96}
}

@ARTICLE{comments-on-software-defects,
    author={Shepperd, Martin and Song, Qinbao and Sun, Zhongbin and Mair, Carolyn},
    journal={IEEE Transactions on Software Engineering},
    title={Data Quality: Some Comments on the NASA Software Defect Datasets},
    year={2013},
    volume={39},
    number={9},
    pages={1208-1215},
    keywords={NASA;Software;PROM;Educational institutions;Sun;Communities;Abstracts;Empirical software engineering;data quality;machine learning;defect prediction},
    doi={10.1109/TSE.2013.11}
}

@article{anchoring-data-quality,
    author = {Wand, Yair and Wang, Richard Y.},
    title = {Anchoring data quality dimensions in ontological foundations},
    year = {1996},
    issue_date = {Nov. 1996},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {39},
    number = {11},
    issn = {0001-0782},
    url = {https://doi.org/10.1145/240455.240479},
    doi = {10.1145/240455.240479},
    journal = {Commun. ACM},
    month = nov,
    pages = {86–95},
    numpages = {10}
}

@article{testing-outlying-observations,
    ISSN = {00034851},
    URL = {http://www.jstor.org/stable/2236553},
    author = {Frank E. Grubbs},
    journal = {The Annals of Mathematical Statistics},
    number = {1},
    pages = {27--58},
    publisher = {Institute of Mathematical Statistics},
    title = {Sample Criteria for Testing Outlying Observations},
    urldate = {2025-06-13},
    volume = {21},
    year = {1950}
}

@INPROCEEDINGS{other-face-of-big-data,
    author={Saha, Barna and Srivastava, Divesh},
    booktitle={2014 IEEE 30th International Conference on Data Engineering}, 
    title={Data quality: The other face of Big Data}, 
    year={2014},
    volume={},
    number={},
    pages={1294-1297},
    keywords={Information management;Data handling;Data storage systems;Databases;Maintenance engineering;Quality management;Cleaning},
    doi={10.1109/ICDE.2014.6816764}
}


@inproceedings{data-linter,
    title	= {The Data Linter: Lightweight Automated Sanity Checking for ML Data Sets},
    author	= {Nick Hynes and D. Sculley and Michael Terry},
    year	= {2017},
    URL	= {http://learningsys.org/nips17/assets/papers/paper_19.pdf}
}

@article{data-quality-info-and-decision-making,
    author = {Kerr, Karolyn and Norris, Tony and Stockdale, Rosemary},
    year = {2007},
    month = {01},
    pages = {},
    title = {Data quality information and decision making: A healthcare case study},
    journal = {ACIS 2007 Proceedings - 18th Australasian Conference on Information Systems}
}

@inproceedings{formal-definition,
    author = {Oliveira, Paulo and Rodrigues, Fátima and Rangel Henriques, Pedro},
    year = {2005},
    month = {01},
    pages = {},
    title = {A Formal Definition of Data Quality Problems.},
    journal = {Proceedings of the 2005 International Conference on Information Quality, ICIQ 2005}
}

@inproceedings{comparison-of-string-distances,
    author = {Cohen, William W. and Ravikumar, Pradeep and Fienberg, Stephen E.},
    title = {A comparison of string distance metrics for name-matching tasks},
    year = {2003},
    publisher = {AAAI Press},
    booktitle = {Proceedings of the 2003 International Conference on Information Integration on the Web},
    pages = {73–78},
    numpages = {6},
    location = {Acapulco, Mexico},
    series = {IIWEB'03}
}

@misc{opendatasus-sim,
    author  = {{Brasil, Ministério da Saúde}},
    title   = {Banco de dados do Sistema Único de Saúde-DATASUS},
    year    = {2024},
    url     = {https://opendatasus.saude.gov.br/pt_BR/dataset/sim},
    note    = {Accessed 8 Jul. 2025}
}

@software{Gong_Great_Expectations,
    author = {Gong, Abe and Campbell, James and {Great Expectations}},
    license = {Apache-2.0},
    title = {{Great Expectations}},
    url = {https://github.com/great-expectations/great_expectations}
}
